[2024-12-24 21:35:50 ViT-B/16] (CL_main_v2.py 442): INFO working dir: ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/
[2024-12-24 21:35:50 ViT-B/16] (CL_main_v2.py 446): INFO AUG:
  COLOR_JITTER: 0.8
  CUTMIX: 1.0
  GRAY_SCALE: 0.2
  LABEL_SMOOTH: 0.1
  MIXUP: 0.8
  MIXUP_SWITCH_PROB: 0.5
BASE: ['']
DATA:
  ASYNC_LOADING: False
  DATASET: ntu60
  INPUT_SIZE: 224
  LABEL_LIST: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_base_class_labels.csv
  NUM_CLASSES: 48
  NUM_FRAMES: 16
  ROOT: /data/ntu/NTU60_224x224/rgb
  SKELETON_DATA: /data/vidlab_datasets/ntu/Hyperformer_processed_data/NTU60_CS.npz
  TRAIN_FILE: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_ntu60_base_train_data_v2.csv
  VAL_FILE: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_ntu60_base_val_data_v2.csv
LOCAL_RANK: 0
MODEL:
  ARCH: ViT-B/16
  DROP_PATH_RATE: 0.0
  FIX_TEXT: True
  GRAPH: graph.ntu_rgb_d.Graph
  JOINT_LABEL: [0, 4, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 1]
  LABELING_MODE: spatial
  POSE_MODEL: trainers.Hyperformer.Hyperformer_Model
  PRETRAINED: None
  RESUME: ./workdirs/TEST_P1_SKeletonCLIP_alltrn_HFinit_zs_48_17Dec24/ckpt_epoch_0.pth
  RESUME_POSE: /data/users/asinha13/projects/CLIP4ADL/model_chkpnts/ntu_CS_P1_HF_pretrain_frzn_txt_zs_48_AS_16Jan24/ckpt_epoch_99.pth
OUTPUT: ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/
PRINT_FREQ: 50
SAVE_FREQ: 25
SEED: 1024
TEST:
  MULTI_VIEW_INFERENCE: False
  NUM_CLIP: 1
  NUM_CROP: 1
  ONLY_TEST: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: False
  BATCH_SIZE: 1
  CLIP_GRAD: 1.0
  EPOCHS: 1
  LOSS_SCALE: 10.0
  LR: 2.2e-05
  LR_SCHEDULER: cosine
  OPTIMIZER: adamw
  OPT_LEVEL: O1
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 0
  WEIGHT_DECAY: 0.001
TRAINER:
  SKI_VLM:
    CTX_INIT: a photo of a
    N_CTX_TEXT: 0
    N_CTX_VISION: 0
    PROMPT_DEPTH_TEXT: 1
    PROMPT_DEPTH_VISION: 0
    PROMPT_MODEL: False
    USE: both
    ZS_EVAL: False
[2024-12-24 21:36:05 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 261): INFO Loading CLIP (backbone: ViT-B/16)
[2024-12-24 21:36:09 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 265): INFO Building Model
[2024-12-24 21:36:09 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 282): INFO Turning on gradients for COMPLETE SKI_VLM model
[2024-12-24 21:36:09 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 315): INFO Parameters to be updated: {'hyperformer_model.l5.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'clip_text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'image_encoder.class_embedding', 'hyperformer_model.l5.residual.bn.weight', 'clip_text_encoder.text_projection', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'clip_text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'hyperformer_model.l1.residual.bn.bias', 'clip_text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'image_encoder.proj', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'clip_text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l6.vit1.attn.outer', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.vit1.attn.w1', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'clip_text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'clip_text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.positional_embedding', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l4.vit1.attn.proj.weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.1.ln_1.weight', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'clip_text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'hyperformer_model.l4.vit1.pe_proj.weight', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'clip_text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l4.vit1.norm1.weight', 'image_encoder.ln_pre.weight', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'hyperformer_model.l1.residual.conv.weight', 'hyperformer_model.l10.vit1.attn.outer', 'clip_text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l5.vit1.attn.q.weight', 'clip_text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'text_encoder.ln_final.weight', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'hyperformer_model.l2.vit1.attn.w1', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.4.ln_2.bias', 'clip_text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l8.vit1.attn.rpe', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.skip_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'clip_text_encoder.transformer.resblocks.2.ln_1.weight', 'clip_text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'clip_text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'clip_text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'hyperformer_model.l8.vit1.attn.outer', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'clip_text_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.fc2.weight', 'hyperformer_model.l7.vit1.attn.alpha', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'clip_text_encoder.transformer.resblocks.0.ln_1.bias', 'clip_text_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l2.vit1.attn.alpha', 'clip_text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'clip_text_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'clip_text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.4.ln_2.weight', 'clip_text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l2.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l1.vit1.norm1.bias', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l9.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l9.vit1.norm1.weight', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.1.ln_2.bias', 'clip_text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'text_encoder.ln_final.bias', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'hyperformer_model.data_bn.bias', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.10.ln_2.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.8.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'image_encoder.ln_pre.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'clip_text_encoder.positional_embedding', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'clip_text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.attn.w1', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.vit1.attn.rpe', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'clip_text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.ln_post.weight', 'hyperformer_model.l5.vit1.attn.w1', 'hyperformer_model.l6.vit1.attn.alpha', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.vit1.attn.outer', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'clip_text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'clip_text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'clip_text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'hyperformer_model.l10.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'clip_text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'hyperformer_model.l3.vit1.attn.w1', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'hyperformer_model.l10.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l4.vit1.attn.outer', 'clip_text_encoder.ln_final.weight', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'clip_text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'hyperformer_model.l3.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l9.vit1.norm1.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'clip_text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'text_encoder.positional_embedding', 'hyperformer_model.l3.vit1.attn.alpha', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'clip_text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l6.vit1.pe_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'clip_text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l9.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'clip_text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.vit1.attn.kv.weight', 'clip_text_encoder.transformer.resblocks.6.ln_2.bias', 'clip_text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'clip_text_encoder.transformer.resblocks.3.ln_2.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'hyperformer_model.l1.vit1.attn.rpe', 'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'clip_text_encoder.transformer.resblocks.6.ln_1.bias', 'clip_text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'hyperformer_model.l8.vit1.attn.w1', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.4.ln_1.bias', 'clip_text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.7.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'hyperformer_model.l2.vit1.norm1.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.10.ln_1.bias', 'clip_text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'clip_text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'clip_text_encoder.transformer.resblocks.4.ln_1.weight', 'clip_text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.conv1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'clip_text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'clip_text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'clip_text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l2.vit1.attn.proj.bias', 'hyperformer_model.l9.vit1.attn.rpe', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.norm1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'clip_text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l3.vit1.norm1.weight', 'clip_text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l1.vit1.attn.outer', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l10.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'clip_text_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.6.ln_2.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l5.residual.conv.bias', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l6.vit1.norm1.weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.0.ln_2.bias', 'clip_text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'clip_text_encoder.transformer.resblocks.7.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'clip_text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'clip_text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'clip_text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.2.ln_2.bias', 'clip_text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l4.vit1.norm1.bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'hyperformer_model.l6.vit1.attn.rpe', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l2.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l7.vit1.attn.outer', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'clip_text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'hyperformer_model.data_bn.weight', 'clip_text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l7.vit1.attn.kv.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l2.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l8.residual.bn.weight', 'hyperformer_model.l1.vit1.attn.kv.weight', 'clip_text_encoder.transformer.resblocks.7.ln_2.weight', 'clip_text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'hyperformer_model.l2.vit1.attn.q.weight', 'hyperformer_model.l8.residual.bn.bias', 'hyperformer_model.l1.residual.bn.weight', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l5.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'clip_text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l5.residual.bn.bias', 'hyperformer_model.l9.vit1.attn.proj.weight', 'clip_text_encoder.transformer.resblocks.9.ln_2.bias', 'clip_text_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l3.vit1.attn.outer', 'hyperformer_model.l6.vit1.attn.proj.weight', 'clip_text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l5.vit1.pe_proj.weight', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'clip_text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l8.vit1.norm1.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l3.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l7.vit1.attn.w1', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'clip_text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l8.vit1.norm1.bias', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.5.ln_1.bias', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'text_encoder.text_projection', 'hyperformer_model.fc2.bias', 'clip_text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.alpha', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.8.ln_1.weight', 'clip_text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'clip_text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'clip_text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l8.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'clip_text_encoder.transformer.resblocks.11.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.2.0.weight', 'hyperformer_model.l4.vit1.attn.alpha', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'clip_text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'clip_text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'hyperformer_model.l8.residual.conv.bias', 'hyperformer_model.l1.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'clip_text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'clip_text_encoder.transformer.resblocks.3.ln_2.bias', 'clip_text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'hyperformer_model.l1.residual.conv.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'clip_text_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'image_encoder.ln_post.bias', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'clip_text_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l10.vit1.pe_proj.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'clip_text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l4.vit1.attn.w1', 'hyperformer_model.l4.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l5.vit1.attn.rpe', 'clip_text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'clip_text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'hyperformer_model.l9.vit1.attn.outer', 'clip_text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight'}
[2024-12-24 21:36:09 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 316): INFO Total learnable items: 835
[2024-12-24 21:36:30 ViT-B/16] (tools.py 147): INFO ==============> Resuming from ./workdirs/TEST_P1_SKeletonCLIP_alltrn_HFinit_zs_48_17Dec24/ckpt_epoch_0.pth....................
[2024-12-24 21:36:31 ViT-B/16] (tools.py 172): INFO resume VIFICLIP model: _IncompatibleKeys(missing_keys=['module.prompt_learner.complete_text_embeddings', 'module.image_encoder.class_embedding', 'module.image_encoder.positional_embedding', 'module.image_encoder.proj', 'module.image_encoder.conv1.weight', 'module.image_encoder.ln_pre.weight', 'module.image_encoder.ln_pre.bias', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_1.weight', 'module.image_encoder.transformer.resblocks.0.ln_1.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_2.weight', 'module.image_encoder.transformer.resblocks.0.ln_2.bias', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_1.weight', 'module.image_encoder.transformer.resblocks.1.ln_1.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_2.weight', 'module.image_encoder.transformer.resblocks.1.ln_2.bias', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_1.weight', 'module.image_encoder.transformer.resblocks.2.ln_1.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_2.weight', 'module.image_encoder.transformer.resblocks.2.ln_2.bias', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_1.weight', 'module.image_encoder.transformer.resblocks.3.ln_1.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_2.weight', 'module.image_encoder.transformer.resblocks.3.ln_2.bias', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_1.weight', 'module.image_encoder.transformer.resblocks.4.ln_1.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_2.weight', 'module.image_encoder.transformer.resblocks.4.ln_2.bias', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_1.weight', 'module.image_encoder.transformer.resblocks.5.ln_1.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_2.weight', 'module.image_encoder.transformer.resblocks.5.ln_2.bias', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_1.weight', 'module.image_encoder.transformer.resblocks.6.ln_1.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_2.weight', 'module.image_encoder.transformer.resblocks.6.ln_2.bias', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_1.weight', 'module.image_encoder.transformer.resblocks.7.ln_1.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_2.weight', 'module.image_encoder.transformer.resblocks.7.ln_2.bias', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_1.weight', 'module.image_encoder.transformer.resblocks.8.ln_1.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_2.weight', 'module.image_encoder.transformer.resblocks.8.ln_2.bias', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_1.weight', 'module.image_encoder.transformer.resblocks.9.ln_1.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_2.weight', 'module.image_encoder.transformer.resblocks.9.ln_2.bias', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_1.weight', 'module.image_encoder.transformer.resblocks.10.ln_1.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_2.weight', 'module.image_encoder.transformer.resblocks.10.ln_2.bias', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_1.weight', 'module.image_encoder.transformer.resblocks.11.ln_1.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_2.weight', 'module.image_encoder.transformer.resblocks.11.ln_2.bias', 'module.image_encoder.ln_post.weight', 'module.image_encoder.ln_post.bias', 'module.clip_text_encoder.positional_embedding', 'module.clip_text_encoder.text_projection', 'module.clip_text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.0.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.0.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.0.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.0.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.1.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.1.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.1.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.1.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.2.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.2.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.2.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.2.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.3.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.3.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.3.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.3.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.4.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.4.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.4.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.4.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.5.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.5.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.5.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.5.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.6.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.6.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.6.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.6.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.7.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.7.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.7.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.7.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.8.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.8.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.8.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.8.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.9.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.9.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.9.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.9.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.10.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.10.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.10.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.10.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.11.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.11.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.11.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.11.ln_2.bias', 'module.clip_text_encoder.ln_final.weight', 'module.clip_text_encoder.ln_final.bias'], unexpected_keys=[])
[2024-12-24 21:36:31 ViT-B/16] (tools.py 176): INFO ==============> Resuming from /data/users/asinha13/projects/CLIP4ADL/model_chkpnts/ntu_CS_P1_HF_pretrain_frzn_txt_zs_48_AS_16Jan24/ckpt_epoch_99.pth....................
[2024-12-24 21:36:31 ViT-B/16] (tools.py 218): INFO resume Hyperformer model: _IncompatibleKeys(missing_keys=['module.prompt_learner.complete_text_embeddings', 'module.image_encoder.class_embedding', 'module.image_encoder.positional_embedding', 'module.image_encoder.proj', 'module.image_encoder.conv1.weight', 'module.image_encoder.ln_pre.weight', 'module.image_encoder.ln_pre.bias', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_1.weight', 'module.image_encoder.transformer.resblocks.0.ln_1.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_2.weight', 'module.image_encoder.transformer.resblocks.0.ln_2.bias', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_1.weight', 'module.image_encoder.transformer.resblocks.1.ln_1.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_2.weight', 'module.image_encoder.transformer.resblocks.1.ln_2.bias', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_1.weight', 'module.image_encoder.transformer.resblocks.2.ln_1.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_2.weight', 'module.image_encoder.transformer.resblocks.2.ln_2.bias', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_1.weight', 'module.image_encoder.transformer.resblocks.3.ln_1.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_2.weight', 'module.image_encoder.transformer.resblocks.3.ln_2.bias', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_1.weight', 'module.image_encoder.transformer.resblocks.4.ln_1.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_2.weight', 'module.image_encoder.transformer.resblocks.4.ln_2.bias', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_1.weight', 'module.image_encoder.transformer.resblocks.5.ln_1.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_2.weight', 'module.image_encoder.transformer.resblocks.5.ln_2.bias', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_1.weight', 'module.image_encoder.transformer.resblocks.6.ln_1.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_2.weight', 'module.image_encoder.transformer.resblocks.6.ln_2.bias', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_1.weight', 'module.image_encoder.transformer.resblocks.7.ln_1.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_2.weight', 'module.image_encoder.transformer.resblocks.7.ln_2.bias', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_1.weight', 'module.image_encoder.transformer.resblocks.8.ln_1.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_2.weight', 'module.image_encoder.transformer.resblocks.8.ln_2.bias', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_1.weight', 'module.image_encoder.transformer.resblocks.9.ln_1.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_2.weight', 'module.image_encoder.transformer.resblocks.9.ln_2.bias', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_1.weight', 'module.image_encoder.transformer.resblocks.10.ln_1.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_2.weight', 'module.image_encoder.transformer.resblocks.10.ln_2.bias', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_1.weight', 'module.image_encoder.transformer.resblocks.11.ln_1.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_2.weight', 'module.image_encoder.transformer.resblocks.11.ln_2.bias', 'module.image_encoder.ln_post.weight', 'module.image_encoder.ln_post.bias', 'module.text_encoder.positional_embedding', 'module.text_encoder.text_projection', 'module.text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.0.ln_1.weight', 'module.text_encoder.transformer.resblocks.0.ln_1.bias', 'module.text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.0.ln_2.weight', 'module.text_encoder.transformer.resblocks.0.ln_2.bias', 'module.text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.1.ln_1.weight', 'module.text_encoder.transformer.resblocks.1.ln_1.bias', 'module.text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.1.ln_2.weight', 'module.text_encoder.transformer.resblocks.1.ln_2.bias', 'module.text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.2.ln_1.weight', 'module.text_encoder.transformer.resblocks.2.ln_1.bias', 'module.text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.2.ln_2.weight', 'module.text_encoder.transformer.resblocks.2.ln_2.bias', 'module.text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.3.ln_1.weight', 'module.text_encoder.transformer.resblocks.3.ln_1.bias', 'module.text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.3.ln_2.weight', 'module.text_encoder.transformer.resblocks.3.ln_2.bias', 'module.text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.4.ln_1.weight', 'module.text_encoder.transformer.resblocks.4.ln_1.bias', 'module.text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.4.ln_2.weight', 'module.text_encoder.transformer.resblocks.4.ln_2.bias', 'module.text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.5.ln_1.weight', 'module.text_encoder.transformer.resblocks.5.ln_1.bias', 'module.text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.5.ln_2.weight', 'module.text_encoder.transformer.resblocks.5.ln_2.bias', 'module.text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.6.ln_1.weight', 'module.text_encoder.transformer.resblocks.6.ln_1.bias', 'module.text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.6.ln_2.weight', 'module.text_encoder.transformer.resblocks.6.ln_2.bias', 'module.text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.7.ln_1.weight', 'module.text_encoder.transformer.resblocks.7.ln_1.bias', 'module.text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.7.ln_2.weight', 'module.text_encoder.transformer.resblocks.7.ln_2.bias', 'module.text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.8.ln_1.weight', 'module.text_encoder.transformer.resblocks.8.ln_1.bias', 'module.text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.8.ln_2.weight', 'module.text_encoder.transformer.resblocks.8.ln_2.bias', 'module.text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.9.ln_1.weight', 'module.text_encoder.transformer.resblocks.9.ln_1.bias', 'module.text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.9.ln_2.weight', 'module.text_encoder.transformer.resblocks.9.ln_2.bias', 'module.text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.10.ln_1.weight', 'module.text_encoder.transformer.resblocks.10.ln_1.bias', 'module.text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.10.ln_2.weight', 'module.text_encoder.transformer.resblocks.10.ln_2.bias', 'module.text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.11.ln_1.weight', 'module.text_encoder.transformer.resblocks.11.ln_1.bias', 'module.text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.11.ln_2.weight', 'module.text_encoder.transformer.resblocks.11.ln_2.bias', 'module.text_encoder.ln_final.weight', 'module.text_encoder.ln_final.bias'], unexpected_keys=[])
[2024-12-24 21:36:36 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][0/11317]	eta 16:16:16 lr 0.000022000	time 5.1759 (5.1759)	dist_loss 0.4401 (0.4401)	image_text_loss 3.8434 (3.8434)	pose_text_loss 3.1540 (3.1540)	tot_loss 11.3981 (11.3981)	mem 29650MB
[2024-12-24 21:36:53 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][50/11317]	eta 1:22:39 lr 0.000021999	time 0.3500 (0.4401)	dist_loss 0.0061 (0.1055)	image_text_loss 3.8884 (3.8754)	pose_text_loss 3.8570 (3.7149)	tot_loss 7.8062 (8.6450)	mem 29650MB
[2024-12-24 21:37:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][100/11317]	eta 1:14:16 lr 0.000021996	time 0.3599 (0.3973)	dist_loss 0.0150 (0.0609)	image_text_loss 3.7708 (3.8737)	pose_text_loss 3.6946 (3.7335)	tot_loss 7.6154 (8.2166)	mem 29650MB
[2024-12-24 21:38:19 ViT-B/16] (CL_main_v2.py 442): INFO working dir: ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/
[2024-12-24 21:38:19 ViT-B/16] (CL_main_v2.py 446): INFO AUG:
  COLOR_JITTER: 0.8
  CUTMIX: 1.0
  GRAY_SCALE: 0.2
  LABEL_SMOOTH: 0.1
  MIXUP: 0.8
  MIXUP_SWITCH_PROB: 0.5
BASE: ['']
DATA:
  ASYNC_LOADING: False
  DATASET: ntu60
  INPUT_SIZE: 224
  LABEL_LIST: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_base_class_labels.csv
  NUM_CLASSES: 48
  NUM_FRAMES: 16
  ROOT: /data/ntu/NTU60_224x224/rgb
  SKELETON_DATA: /data/vidlab_datasets/ntu/Hyperformer_processed_data/NTU60_CS.npz
  TRAIN_FILE: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_ntu60_base_train_data_v2.csv
  VAL_FILE: /data/users/asinha13/ntu/ntu60_cs_zero_shot_AS_2Nov23/zsl_48-12_ntu60_base_val_data_v2.csv
LOCAL_RANK: 0
MODEL:
  ARCH: ViT-B/16
  DROP_PATH_RATE: 0.0
  FIX_TEXT: True
  GRAPH: graph.ntu_rgb_d.Graph
  JOINT_LABEL: [0, 4, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 3, 3, 3, 2, 3, 3, 3, 1, 0, 1, 0, 1]
  LABELING_MODE: spatial
  POSE_MODEL: trainers.Hyperformer.Hyperformer_Model
  PRETRAINED: None
  RESUME: ./workdirs/TEST_P1_SKeletonCLIP_alltrn_HFinit_zs_48_17Dec24/ckpt_epoch_0.pth
  RESUME_POSE: /data/users/asinha13/projects/CLIP4ADL/model_chkpnts/ntu_CS_P1_HF_pretrain_frzn_txt_zs_48_AS_16Jan24/ckpt_epoch_99.pth
OUTPUT: ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/
PRINT_FREQ: 50
SAVE_FREQ: 25
SEED: 1024
TEST:
  MULTI_VIEW_INFERENCE: False
  NUM_CLIP: 1
  NUM_CROP: 1
  ONLY_TEST: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: False
  BATCH_SIZE: 1
  CLIP_GRAD: 1.0
  EPOCHS: 1
  LOSS_SCALE: 10.0
  LR: 2.2e-05
  LR_SCHEDULER: cosine
  OPTIMIZER: adamw
  OPT_LEVEL: O1
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 0
  WEIGHT_DECAY: 0.001
TRAINER:
  SKI_VLM:
    CTX_INIT: a photo of a
    N_CTX_TEXT: 0
    N_CTX_VISION: 0
    PROMPT_DEPTH_TEXT: 1
    PROMPT_DEPTH_VISION: 0
    PROMPT_MODEL: False
    USE: both
    ZS_EVAL: False
[2024-12-24 21:39:08 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 261): INFO Loading CLIP (backbone: ViT-B/16)
[2024-12-24 21:39:12 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 265): INFO Building Model
[2024-12-24 21:39:12 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 282): INFO Turning on gradients for COMPLETE SKI_VLM model
[2024-12-24 21:39:12 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 315): INFO Parameters to be updated: {'text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l5.vit1.attn.proj.bias', 'hyperformer_model.l3.tcn1.branches.1.0.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.0.ln_2.bias', 'hyperformer_model.l3.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.data_bn.weight', 'text_encoder.transformer.resblocks.5.ln_1.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.1.0.bias', 'hyperformer_model.l9.tcn1.branches.0.1.weight', 'hyperformer_model.l2.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l2.tcn1.branches.1.0.bias', 'hyperformer_model.l5.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l2.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l5.residual.conv.weight', 'hyperformer_model.l2.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_2.weight', 'clip_text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l6.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.3.ln_2.bias', 'hyperformer_model.l2.vit1.attn.rpe', 'hyperformer_model.l4.vit1.attn.w1', 'hyperformer_model.l3.vit1.attn.outer', 'text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'text_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_1.weight', 'text_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.0.1.bias', 'hyperformer_model.l8.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l9.vit1.attn.rpe', 'image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l10.tcn1.branches.2.1.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l1.residual.bn.bias', 'image_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.4.weight', 'hyperformer_model.l7.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.10.ln_1.bias', 'image_encoder.transformer.resblocks.0.ln_1.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.ln_2.weight', 'text_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.7.ln_1.weight', 'text_encoder.ln_final.bias', 'image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'hyperformer_model.l7.vit1.attn.proj.weight', 'hyperformer_model.l6.vit1.pe_proj.weight', 'image_encoder.class_embedding', 'clip_text_encoder.transformer.resblocks.3.ln_2.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'hyperformer_model.l8.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.1.ln_1.bias', 'text_encoder.transformer.resblocks.9.ln_1.weight', 'image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'image_encoder.transformer.resblocks.4.ln_1.weight', 'clip_text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l2.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'hyperformer_model.l3.vit1.attn.q.weight', 'hyperformer_model.l8.tcn1.branches.0.0.bias', 'hyperformer_model.l8.tcn1.branches.2.1.bias', 'hyperformer_model.l10.tcn1.branches.0.3.bn.bias', 'text_encoder.transformer.resblocks.2.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.2.0.weight', 'clip_text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.2.ln_1.weight', 'image_encoder.transformer.resblocks.5.ln_1.weight', 'clip_text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.0.3.conv.weight', 'hyperformer_model.fc2.weight', 'text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.weight', 'text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.ln_1.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l5.vit1.attn.q.weight', 'hyperformer_model.l4.vit1.norm1.bias', 'clip_text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'hyperformer_model.l5.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.1.ln_1.weight', 'hyperformer_model.l6.tcn1.branches.3.0.bias', 'text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'hyperformer_model.l8.tcn1.branches.2.4.bias', 'image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.0.weight', 'hyperformer_model.l1.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.0.1.bias', 'hyperformer_model.l9.tcn1.branches.1.1.weight', 'hyperformer_model.l6.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.4.ln_1.bias', 'clip_text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.weight', 'clip_text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l7.vit1.norm1.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l1.tcn1.branches.0.0.bias', 'hyperformer_model.l3.tcn1.branches.3.1.weight', 'hyperformer_model.l6.vit1.attn.outer', 'image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l8.residual.conv.weight', 'hyperformer_model.l6.vit1.attn.alpha', 'hyperformer_model.l9.vit1.norm1.bias', 'image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.1.bias', 'hyperformer_model.l7.vit1.attn.proj.bias', 'text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.tcn1.branches.3.1.bias', 'hyperformer_model.l2.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.3.ln_1.bias', 'clip_text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'hyperformer_model.l7.tcn1.branches.3.1.weight', 'hyperformer_model.l9.tcn1.branches.2.4.bias', 'hyperformer_model.l1.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l5.vit1.pe_proj.weight', 'clip_text_encoder.transformer.resblocks.0.ln_2.weight', 'text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l6.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l4.tcn1.branches.3.1.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l10.vit1.norm1.bias', 'hyperformer_model.l10.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.2.ln_1.weight', 'clip_text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.l4.tcn1.branches.2.4.weight', 'hyperformer_model.l9.vit1.pe_proj.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'hyperformer_model.l2.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.ln_2.weight', 'clip_text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l7.tcn1.branches.2.4.bias', 'clip_text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l7.tcn1.branches.3.0.bias', 'clip_text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.4.ln_2.weight', 'clip_text_encoder.transformer.resblocks.4.ln_2.weight', 'text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l7.tcn1.branches.0.0.weight', 'hyperformer_model.l5.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.2.ln_2.weight', 'text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l8.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.0.0.weight', 'hyperformer_model.l3.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l4.tcn1.branches.1.0.bias', 'clip_text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.0.weight', 'text_encoder.transformer.resblocks.3.ln_1.bias', 'text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.ln_1.bias', 'hyperformer_model.l2.vit1.attn.outer', 'hyperformer_model.l3.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l9.vit1.attn.proj.weight', 'hyperformer_model.l5.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l3.tcn1.branches.0.3.bn.weight', 'clip_text_encoder.transformer.resblocks.10.ln_2.bias', 'clip_text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'hyperformer_model.l3.tcn1.branches.0.0.weight', 'hyperformer_model.l5.vit1.norm1.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.weight', 'text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'hyperformer_model.l2.vit1.attn.alpha', 'hyperformer_model.l1.tcn1.branches.0.0.weight', 'hyperformer_model.l5.vit1.attn.w1', 'image_encoder.ln_pre.bias', 'clip_text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.0.1.bias', 'hyperformer_model.l6.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.1.1.weight', 'clip_text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.2.0.weight', 'hyperformer_model.l4.tcn1.branches.3.0.bias', 'hyperformer_model.l8.vit1.norm1.bias', 'hyperformer_model.l3.vit1.attn.alpha', 'text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'clip_text_encoder.text_projection', 'hyperformer_model.l1.tcn1.branches.3.0.weight', 'hyperformer_model.l8.tcn1.branches.1.1.bias', 'text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'hyperformer_model.l7.vit1.attn.rpe', 'text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'hyperformer_model.l6.tcn1.branches.3.0.weight', 'text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.8.ln_1.weight', 'hyperformer_model.l8.residual.conv.bias', 'hyperformer_model.l1.tcn1.branches.1.1.weight', 'hyperformer_model.l4.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.4.ln_1.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'hyperformer_model.l2.tcn1.branches.1.3.conv.weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.1.0.bias', 'hyperformer_model.l6.tcn1.branches.2.0.weight', 'hyperformer_model.l7.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.3.1.weight', 'image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'hyperformer_model.l10.vit1.attn.w1', 'text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l10.tcn1.branches.1.1.bias', 'hyperformer_model.l2.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.3.1.bias', 'hyperformer_model.l9.tcn1.branches.2.0.bias', 'hyperformer_model.l3.tcn1.branches.0.1.weight', 'hyperformer_model.l10.vit1.attn.alpha', 'clip_text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'hyperformer_model.l10.tcn1.branches.2.4.weight', 'hyperformer_model.l8.vit1.attn.proj.bias', 'hyperformer_model.l10.tcn1.branches.3.0.weight', 'hyperformer_model.l4.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.1.0.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l3.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l8.vit1.attn.rpe', 'hyperformer_model.l6.tcn1.branches.2.0.bias', 'hyperformer_model.l5.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l8.vit1.attn.w1', 'hyperformer_model.l10.vit1.attn.proj.bias', 'hyperformer_model.l4.vit1.attn.outer', 'text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l10.tcn1.branches.1.0.bias', 'clip_text_encoder.transformer.resblocks.5.ln_1.bias', 'hyperformer_model.l4.vit1.pe_proj.weight', 'clip_text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l2.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'hyperformer_model.l1.tcn1.branches.1.0.bias', 'clip_text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'text_encoder.text_projection', 'text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'clip_text_encoder.ln_final.weight', 'hyperformer_model.l4.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.4.ln_2.bias', 'hyperformer_model.l10.tcn1.branches.3.1.bias', 'clip_text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l6.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.3.0.weight', 'image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l9.tcn1.branches.0.0.bias', 'hyperformer_model.l10.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.1.ln_2.weight', 'clip_text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l1.residual.conv.bias', 'hyperformer_model.l5.residual.bn.bias', 'clip_text_encoder.ln_final.bias', 'text_encoder.transformer.resblocks.11.ln_2.bias', 'text_encoder.ln_final.weight', 'clip_text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l4.vit1.attn.kv.weight', 'hyperformer_model.l7.vit1.pe_proj.weight', 'hyperformer_model.l8.tcn1.branches.0.1.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l5.tcn1.branches.2.1.weight', 'hyperformer_model.l5.vit1.norm1.bias', 'hyperformer_model.l6.vit1.attn.q.weight', 'hyperformer_model.l3.tcn1.branches.2.1.bias', 'clip_text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.7.ln_1.bias', 'text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.0.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.vit1.attn.outer', 'image_encoder.transformer.resblocks.6.ln_2.bias', 'image_encoder.positional_embedding', 'image_encoder.ln_pre.weight', 'clip_text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'hyperformer_model.l2.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l6.vit1.norm1.bias', 'hyperformer_model.data_bn.bias', 'hyperformer_model.l5.tcn1.branches.1.1.bias', 'hyperformer_model.l1.vit1.pe_proj.weight', 'hyperformer_model.l6.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l6.vit1.norm1.weight', 'hyperformer_model.l10.tcn1.branches.0.1.weight', 'clip_text_encoder.transformer.resblocks.11.ln_2.bias', 'image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.0.ln_1.weight', 'hyperformer_model.l5.residual.bn.weight', 'image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.3.1.weight', 'text_encoder.transformer.resblocks.6.ln_1.weight', 'hyperformer_model.l7.vit1.attn.kv.weight', 'clip_text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.1.weight', 'hyperformer_model.l5.tcn1.branches.2.4.bias', 'hyperformer_model.l8.vit1.norm1.weight', 'image_encoder.transformer.resblocks.5.ln_2.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.attn.alpha', 'hyperformer_model.l10.vit1.attn.proj.weight', 'clip_text_encoder.transformer.resblocks.4.ln_1.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'hyperformer_model.l6.tcn1.branches.2.1.bias', 'hyperformer_model.l1.tcn1.branches.1.3.conv.weight', 'clip_text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'image_encoder.transformer.resblocks.1.ln_1.weight', 'text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.0.bias', 'text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'hyperformer_model.l4.tcn1.branches.2.1.bias', 'hyperformer_model.l4.tcn1.branches.0.1.weight', 'clip_text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.4.ln_2.bias', 'image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.3.0.bias', 'hyperformer_model.l8.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l5.vit1.attn.proj.weight', 'hyperformer_model.l9.tcn1.branches.1.0.bias', 'hyperformer_model.l4.tcn1.branches.1.1.bias', 'hyperformer_model.l4.vit1.attn.q.weight', 'hyperformer_model.l5.residual.conv.bias', 'hyperformer_model.l4.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'hyperformer_model.l3.tcn1.branches.2.4.bias', 'hyperformer_model.l7.tcn1.branches.2.1.bias', 'text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l2.vit1.norm1.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l1.vit1.skip_proj.weight', 'clip_text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.9.ln_2.bias', 'clip_text_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'text_encoder.positional_embedding', 'text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'hyperformer_model.l2.tcn1.branches.3.1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l1.vit1.attn.q.weight', 'clip_text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l9.vit1.norm1.weight', 'hyperformer_model.l3.tcn1.branches.0.0.bias', 'image_encoder.transformer.resblocks.3.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_2.bias', 'text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.0.3.conv.bias', 'image_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l5.tcn1.branches.3.0.weight', 'hyperformer_model.l10.vit1.attn.rpe', 'hyperformer_model.l1.vit1.attn.alpha', 'clip_text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l3.vit1.norm1.weight', 'hyperformer_model.l2.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.5.ln_1.weight', 'text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'hyperformer_model.l2.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.0.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.3.0.weight', 'hyperformer_model.l8.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.10.ln_1.weight', 'text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.2.4.weight', 'hyperformer_model.l6.tcn1.branches.1.0.bias', 'hyperformer_model.l4.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.1.ln_2.weight', 'hyperformer_model.l7.vit1.attn.alpha', 'text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.7.ln_2.weight', 'hyperformer_model.l10.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'hyperformer_model.l6.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l4.vit1.attn.rpe', 'hyperformer_model.l7.tcn1.branches.2.1.weight', 'image_encoder.transformer.resblocks.0.ln_2.bias', 'clip_text_encoder.transformer.resblocks.0.ln_2.bias', 'image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.11.ln_2.weight', 'image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'hyperformer_model.l9.tcn1.branches.3.1.bias', 'clip_text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'hyperformer_model.l1.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l3.tcn1.branches.1.3.conv.bias', 'hyperformer_model.l8.vit1.attn.q.weight', 'text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.3.ln_1.weight', 'text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l4.tcn1.branches.2.4.bias', 'hyperformer_model.l3.tcn1.branches.3.0.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l5.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.7.ln_1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l10.tcn1.branches.2.1.weight', 'hyperformer_model.l1.tcn1.branches.1.0.weight', 'image_encoder.transformer.resblocks.9.ln_1.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l10.vit1.attn.kv.weight', 'image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l5.tcn1.branches.2.4.weight', 'hyperformer_model.l10.vit1.norm1.weight', 'text_encoder.transformer.resblocks.6.ln_2.weight', 'hyperformer_model.l4.vit1.attn.proj.bias', 'hyperformer_model.l5.vit1.attn.alpha', 'hyperformer_model.l1.vit1.attn.kv.weight', 'text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l6.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'hyperformer_model.l5.vit1.attn.rpe', 'text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.3.conv.weight', 'hyperformer_model.l4.vit1.attn.alpha', 'clip_text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.1.bias', 'hyperformer_model.l8.residual.bn.weight', 'text_encoder.transformer.resblocks.3.ln_1.weight', 'hyperformer_model.l1.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l10.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.8.ln_2.weight', 'text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'hyperformer_model.l8.tcn1.branches.2.0.bias', 'hyperformer_model.l4.vit1.attn.proj.weight', 'clip_text_encoder.transformer.resblocks.8.ln_1.bias', 'text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'hyperformer_model.l4.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.rpe', 'clip_text_encoder.transformer.resblocks.1.ln_2.bias', 'hyperformer_model.l9.vit1.attn.proj.bias', 'hyperformer_model.l7.vit1.attn.q.weight', 'clip_text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.8.ln_1.weight', 'image_encoder.transformer.resblocks.7.ln_1.bias', 'clip_text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'hyperformer_model.l3.vit1.attn.kv.weight', 'hyperformer_model.l5.vit1.attn.outer', 'hyperformer_model.l5.tcn1.branches.2.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.weight', 'hyperformer_model.l7.tcn1.branches.1.1.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'hyperformer_model.l7.tcn1.branches.1.0.weight', 'hyperformer_model.l5.tcn1.branches.0.1.weight', 'hyperformer_model.l6.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.1.1.bias', 'hyperformer_model.l2.vit1.attn.kv.weight', 'clip_text_encoder.transformer.resblocks.2.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.1.1.bias', 'image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.bias', 'clip_text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'hyperformer_model.l8.residual.bn.bias', 'image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.9.ln_1.bias', 'image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.10.ln_1.weight', 'hyperformer_model.l2.vit1.attn.proj.weight', 'clip_text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l6.vit1.attn.kv.weight', 'hyperformer_model.l9.vit1.attn.w1', 'text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'hyperformer_model.l4.tcn1.branches.1.3.bn.weight', 'image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.2.ln_1.bias', 'hyperformer_model.l5.tcn1.branches.2.0.bias', 'hyperformer_model.l1.residual.bn.weight', 'image_encoder.transformer.resblocks.4.ln_2.weight', 'hyperformer_model.l9.tcn1.branches.0.3.conv.bias', 'text_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'hyperformer_model.l8.vit1.attn.proj.weight', 'hyperformer_model.l7.vit1.norm1.bias', 'hyperformer_model.l1.residual.conv.weight', 'text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'hyperformer_model.l7.vit1.attn.outer', 'text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'hyperformer_model.l3.tcn1.branches.3.1.bias', 'hyperformer_model.l7.tcn1.branches.1.3.conv.bias', 'text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l1.tcn1.branches.3.1.weight', 'clip_text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'hyperformer_model.l1.tcn1.branches.2.1.bias', 'hyperformer_model.l3.tcn1.branches.1.1.bias', 'hyperformer_model.l4.tcn1.branches.2.0.weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.bias', 'image_encoder.ln_post.bias', 'hyperformer_model.l2.vit1.pe_proj.weight', 'clip_text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'image_encoder.proj', 'hyperformer_model.l10.vit1.attn.q.weight', 'image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'image_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l8.tcn1.branches.1.3.conv.weight', 'clip_text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.1.0.weight', 'hyperformer_model.l5.tcn1.branches.1.3.conv.bias', 'image_encoder.conv1.weight', 'image_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.8.ln_2.bias', 'text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'hyperformer_model.l3.vit1.attn.proj.bias', 'clip_text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.attn.w1', 'clip_text_encoder.transformer.resblocks.3.ln_1.bias', 'hyperformer_model.l5.vit1.attn.kv.weight', 'clip_text_encoder.positional_embedding', 'clip_text_encoder.transformer.resblocks.2.ln_1.weight', 'clip_text_encoder.transformer.resblocks.6.ln_1.bias', 'image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'image_encoder.transformer.resblocks.7.ln_2.bias', 'text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'hyperformer_model.l9.tcn1.branches.0.0.weight', 'image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'text_encoder.transformer.resblocks.9.ln_2.bias', 'clip_text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'hyperformer_model.l7.tcn1.branches.2.0.weight', 'hyperformer_model.l1.vit1.attn.rpe', 'hyperformer_model.l9.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.5.ln_2.weight', 'hyperformer_model.l8.vit1.attn.alpha', 'text_encoder.transformer.resblocks.5.ln_2.bias', 'hyperformer_model.l1.vit1.attn.outer', 'hyperformer_model.l2.tcn1.branches.1.3.conv.bias', 'image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'hyperformer_model.l2.tcn1.branches.2.4.bias', 'hyperformer_model.l9.tcn1.branches.2.1.weight', 'hyperformer_model.l5.tcn1.branches.0.0.bias', 'text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'hyperformer_model.l4.vit1.norm1.weight', 'image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.0.1.bias', 'clip_text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'hyperformer_model.l9.tcn1.branches.2.1.bias', 'hyperformer_model.l1.vit1.norm1.bias', 'text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'hyperformer_model.l2.vit1.norm1.weight', 'clip_text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'image_encoder.ln_post.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'hyperformer_model.fc2.bias', 'hyperformer_model.l9.vit1.attn.q.weight', 'clip_text_encoder.transformer.resblocks.9.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.1.3.bn.bias', 'text_encoder.transformer.resblocks.6.ln_1.bias', 'text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'hyperformer_model.l9.tcn1.branches.1.3.bn.weight', 'hyperformer_model.l7.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.proj.weight', 'hyperformer_model.l10.tcn1.branches.2.0.bias', 'image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.2.4.weight', 'image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.7.ln_1.bias', 'hyperformer_model.l1.tcn1.branches.2.4.bias', 'hyperformer_model.l3.vit1.pe_proj.weight', 'text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'hyperformer_model.l9.tcn1.branches.3.0.bias', 'hyperformer_model.l10.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.vit1.attn.proj.weight', 'hyperformer_model.l8.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l4.tcn1.branches.1.3.conv.weight', 'image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.8.ln_2.weight', 'image_encoder.transformer.resblocks.4.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.2.4.weight', 'clip_text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'hyperformer_model.l5.tcn1.branches.0.3.bn.weight', 'image_encoder.transformer.resblocks.6.ln_1.bias', 'clip_text_encoder.transformer.resblocks.11.ln_1.weight', 'hyperformer_model.l10.tcn1.branches.0.1.bias', 'image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.2.ln_2.weight', 'clip_text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'hyperformer_model.l5.tcn1.branches.3.0.bias', 'hyperformer_model.l7.tcn1.branches.2.0.bias', 'text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'clip_text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l4.tcn1.branches.3.1.weight', 'hyperformer_model.l6.tcn1.branches.1.3.conv.weight', 'hyperformer_model.l4.tcn1.branches.0.3.conv.weight', 'clip_text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'image_encoder.transformer.resblocks.9.ln_2.bias', 'image_encoder.transformer.resblocks.1.ln_2.bias', 'image_encoder.transformer.resblocks.10.ln_2.bias', 'text_encoder.transformer.resblocks.10.ln_2.weight', 'text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'clip_text_encoder.transformer.resblocks.9.ln_1.weight', 'hyperformer_model.l7.tcn1.branches.0.0.bias', 'hyperformer_model.l7.tcn1.branches.2.4.weight', 'hyperformer_model.l2.vit1.attn.proj.bias', 'image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.ln_1.weight', 'text_encoder.transformer.resblocks.8.ln_2.bias', 'hyperformer_model.l6.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l3.vit1.attn.w1', 'clip_text_encoder.transformer.resblocks.5.ln_1.weight', 'image_encoder.transformer.resblocks.3.ln_2.weight', 'hyperformer_model.l4.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l10.tcn1.branches.1.1.weight', 'image_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.attn.w1', 'hyperformer_model.l1.tcn1.branches.2.1.weight', 'text_encoder.transformer.resblocks.2.ln_2.bias', 'image_encoder.transformer.resblocks.5.ln_1.bias', 'clip_text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'hyperformer_model.l8.tcn1.branches.0.3.bn.weight', 'hyperformer_model.l8.tcn1.branches.1.1.weight', 'hyperformer_model.l9.tcn1.branches.1.3.bn.bias', 'hyperformer_model.l3.tcn1.branches.2.1.weight', 'hyperformer_model.l9.tcn1.branches.3.1.weight', 'hyperformer_model.l7.tcn1.branches.1.0.bias', 'image_encoder.transformer.resblocks.9.ln_2.weight', 'clip_text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'hyperformer_model.l1.vit1.attn.proj.bias', 'hyperformer_model.l9.tcn1.branches.1.3.conv.bias', 'clip_text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'clip_text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.7.ln_2.bias', 'clip_text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'clip_text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'hyperformer_model.l3.vit1.norm1.bias', 'hyperformer_model.l6.vit1.attn.rpe', 'clip_text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.1.ln_1.bias', 'hyperformer_model.l10.tcn1.branches.2.4.bias', 'clip_text_encoder.transformer.resblocks.11.ln_1.bias', 'hyperformer_model.l9.tcn1.branches.0.3.bn.bias', 'hyperformer_model.l9.vit1.attn.kv.weight', 'hyperformer_model.l10.vit1.attn.outer', 'clip_text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l1.tcn1.branches.0.1.weight', 'text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'hyperformer_model.l3.tcn1.branches.2.0.bias', 'clip_text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.2.0.weight', 'clip_text_encoder.transformer.resblocks.6.ln_2.bias', 'hyperformer_model.l7.vit1.attn.w1', 'clip_text_encoder.transformer.resblocks.10.ln_1.bias', 'hyperformer_model.l6.tcn1.branches.1.3.conv.bias', 'clip_text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'clip_text_encoder.transformer.resblocks.4.ln_1.bias', 'text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'clip_text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'hyperformer_model.l6.tcn1.branches.1.1.weight', 'clip_text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'text_encoder.transformer.resblocks.9.ln_2.weight', 'text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.10.ln_2.weight', 'clip_text_encoder.transformer.resblocks.4.ln_2.bias', 'clip_text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'hyperformer_model.l5.tcn1.branches.3.1.bias', 'text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'hyperformer_model.l1.tcn1.branches.3.0.bias', 'hyperformer_model.l5.tcn1.branches.1.1.weight', 'clip_text_encoder.transformer.resblocks.10.ln_1.weight', 'image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'hyperformer_model.l6.vit1.attn.proj.bias', 'hyperformer_model.l8.tcn1.branches.1.0.weight', 'text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'hyperformer_model.l6.tcn1.branches.2.4.weight', 'text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'hyperformer_model.l9.tcn1.branches.3.0.weight', 'clip_text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'hyperformer_model.l1.vit1.attn.w1', 'clip_text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'clip_text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'hyperformer_model.l9.vit1.attn.outer', 'image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'clip_text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'text_encoder.transformer.resblocks.0.ln_1.bias', 'hyperformer_model.l2.tcn1.branches.2.0.weight', 'hyperformer_model.l3.tcn1.branches.3.0.weight', 'hyperformer_model.l6.tcn1.branches.0.1.weight', 'hyperformer_model.l10.tcn1.branches.0.3.conv.bias', 'hyperformer_model.l1.tcn1.branches.2.0.weight'}
[2024-12-24 21:39:12 ViT-B/16] (CL_vificlip_pose_text_merged_model_AS_7Feb24.py 316): INFO Total learnable items: 835
[2024-12-24 21:40:13 ViT-B/16] (tools.py 147): INFO ==============> Resuming from ./workdirs/TEST_P1_SKeletonCLIP_alltrn_HFinit_zs_48_17Dec24/ckpt_epoch_0.pth....................
[2024-12-24 21:40:14 ViT-B/16] (tools.py 172): INFO resume VIFICLIP model: _IncompatibleKeys(missing_keys=['module.prompt_learner.complete_text_embeddings', 'module.image_encoder.class_embedding', 'module.image_encoder.positional_embedding', 'module.image_encoder.proj', 'module.image_encoder.conv1.weight', 'module.image_encoder.ln_pre.weight', 'module.image_encoder.ln_pre.bias', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_1.weight', 'module.image_encoder.transformer.resblocks.0.ln_1.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_2.weight', 'module.image_encoder.transformer.resblocks.0.ln_2.bias', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_1.weight', 'module.image_encoder.transformer.resblocks.1.ln_1.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_2.weight', 'module.image_encoder.transformer.resblocks.1.ln_2.bias', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_1.weight', 'module.image_encoder.transformer.resblocks.2.ln_1.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_2.weight', 'module.image_encoder.transformer.resblocks.2.ln_2.bias', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_1.weight', 'module.image_encoder.transformer.resblocks.3.ln_1.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_2.weight', 'module.image_encoder.transformer.resblocks.3.ln_2.bias', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_1.weight', 'module.image_encoder.transformer.resblocks.4.ln_1.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_2.weight', 'module.image_encoder.transformer.resblocks.4.ln_2.bias', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_1.weight', 'module.image_encoder.transformer.resblocks.5.ln_1.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_2.weight', 'module.image_encoder.transformer.resblocks.5.ln_2.bias', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_1.weight', 'module.image_encoder.transformer.resblocks.6.ln_1.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_2.weight', 'module.image_encoder.transformer.resblocks.6.ln_2.bias', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_1.weight', 'module.image_encoder.transformer.resblocks.7.ln_1.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_2.weight', 'module.image_encoder.transformer.resblocks.7.ln_2.bias', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_1.weight', 'module.image_encoder.transformer.resblocks.8.ln_1.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_2.weight', 'module.image_encoder.transformer.resblocks.8.ln_2.bias', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_1.weight', 'module.image_encoder.transformer.resblocks.9.ln_1.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_2.weight', 'module.image_encoder.transformer.resblocks.9.ln_2.bias', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_1.weight', 'module.image_encoder.transformer.resblocks.10.ln_1.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_2.weight', 'module.image_encoder.transformer.resblocks.10.ln_2.bias', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_1.weight', 'module.image_encoder.transformer.resblocks.11.ln_1.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_2.weight', 'module.image_encoder.transformer.resblocks.11.ln_2.bias', 'module.image_encoder.ln_post.weight', 'module.image_encoder.ln_post.bias', 'module.clip_text_encoder.positional_embedding', 'module.clip_text_encoder.text_projection', 'module.clip_text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.0.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.0.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.0.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.0.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.1.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.1.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.1.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.1.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.2.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.2.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.2.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.2.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.3.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.3.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.3.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.3.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.4.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.4.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.4.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.4.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.5.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.5.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.5.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.5.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.6.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.6.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.6.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.6.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.7.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.7.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.7.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.7.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.8.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.8.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.8.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.8.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.9.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.9.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.9.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.9.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.10.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.10.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.10.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.10.ln_2.bias', 'module.clip_text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.clip_text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.clip_text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.clip_text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.clip_text_encoder.transformer.resblocks.11.ln_1.weight', 'module.clip_text_encoder.transformer.resblocks.11.ln_1.bias', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.clip_text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.clip_text_encoder.transformer.resblocks.11.ln_2.weight', 'module.clip_text_encoder.transformer.resblocks.11.ln_2.bias', 'module.clip_text_encoder.ln_final.weight', 'module.clip_text_encoder.ln_final.bias'], unexpected_keys=[])
[2024-12-24 21:40:14 ViT-B/16] (tools.py 176): INFO ==============> Resuming from /data/users/asinha13/projects/CLIP4ADL/model_chkpnts/ntu_CS_P1_HF_pretrain_frzn_txt_zs_48_AS_16Jan24/ckpt_epoch_99.pth....................
[2024-12-24 21:40:14 ViT-B/16] (tools.py 218): INFO resume Hyperformer model: _IncompatibleKeys(missing_keys=['module.prompt_learner.complete_text_embeddings', 'module.image_encoder.class_embedding', 'module.image_encoder.positional_embedding', 'module.image_encoder.proj', 'module.image_encoder.conv1.weight', 'module.image_encoder.ln_pre.weight', 'module.image_encoder.ln_pre.bias', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_1.weight', 'module.image_encoder.transformer.resblocks.0.ln_1.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.0.ln_2.weight', 'module.image_encoder.transformer.resblocks.0.ln_2.bias', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_1.weight', 'module.image_encoder.transformer.resblocks.1.ln_1.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.1.ln_2.weight', 'module.image_encoder.transformer.resblocks.1.ln_2.bias', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_1.weight', 'module.image_encoder.transformer.resblocks.2.ln_1.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.2.ln_2.weight', 'module.image_encoder.transformer.resblocks.2.ln_2.bias', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_1.weight', 'module.image_encoder.transformer.resblocks.3.ln_1.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.3.ln_2.weight', 'module.image_encoder.transformer.resblocks.3.ln_2.bias', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_1.weight', 'module.image_encoder.transformer.resblocks.4.ln_1.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.4.ln_2.weight', 'module.image_encoder.transformer.resblocks.4.ln_2.bias', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_1.weight', 'module.image_encoder.transformer.resblocks.5.ln_1.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.5.ln_2.weight', 'module.image_encoder.transformer.resblocks.5.ln_2.bias', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_1.weight', 'module.image_encoder.transformer.resblocks.6.ln_1.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.6.ln_2.weight', 'module.image_encoder.transformer.resblocks.6.ln_2.bias', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_1.weight', 'module.image_encoder.transformer.resblocks.7.ln_1.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.7.ln_2.weight', 'module.image_encoder.transformer.resblocks.7.ln_2.bias', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_1.weight', 'module.image_encoder.transformer.resblocks.8.ln_1.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.8.ln_2.weight', 'module.image_encoder.transformer.resblocks.8.ln_2.bias', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_1.weight', 'module.image_encoder.transformer.resblocks.9.ln_1.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.9.ln_2.weight', 'module.image_encoder.transformer.resblocks.9.ln_2.bias', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_1.weight', 'module.image_encoder.transformer.resblocks.10.ln_1.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.10.ln_2.weight', 'module.image_encoder.transformer.resblocks.10.ln_2.bias', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.image_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.image_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_1.weight', 'module.image_encoder.transformer.resblocks.11.ln_1.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.image_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.image_encoder.transformer.resblocks.11.ln_2.weight', 'module.image_encoder.transformer.resblocks.11.ln_2.bias', 'module.image_encoder.ln_post.weight', 'module.image_encoder.ln_post.bias', 'module.text_encoder.positional_embedding', 'module.text_encoder.text_projection', 'module.text_encoder.transformer.resblocks.0.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.0.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.0.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.0.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.0.ln_1.weight', 'module.text_encoder.transformer.resblocks.0.ln_1.bias', 'module.text_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.0.ln_2.weight', 'module.text_encoder.transformer.resblocks.0.ln_2.bias', 'module.text_encoder.transformer.resblocks.1.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.1.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.1.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.1.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.1.ln_1.weight', 'module.text_encoder.transformer.resblocks.1.ln_1.bias', 'module.text_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.1.ln_2.weight', 'module.text_encoder.transformer.resblocks.1.ln_2.bias', 'module.text_encoder.transformer.resblocks.2.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.2.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.2.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.2.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.2.ln_1.weight', 'module.text_encoder.transformer.resblocks.2.ln_1.bias', 'module.text_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.2.ln_2.weight', 'module.text_encoder.transformer.resblocks.2.ln_2.bias', 'module.text_encoder.transformer.resblocks.3.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.3.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.3.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.3.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.3.ln_1.weight', 'module.text_encoder.transformer.resblocks.3.ln_1.bias', 'module.text_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.3.ln_2.weight', 'module.text_encoder.transformer.resblocks.3.ln_2.bias', 'module.text_encoder.transformer.resblocks.4.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.4.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.4.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.4.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.4.ln_1.weight', 'module.text_encoder.transformer.resblocks.4.ln_1.bias', 'module.text_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.4.ln_2.weight', 'module.text_encoder.transformer.resblocks.4.ln_2.bias', 'module.text_encoder.transformer.resblocks.5.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.5.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.5.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.5.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.5.ln_1.weight', 'module.text_encoder.transformer.resblocks.5.ln_1.bias', 'module.text_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.5.ln_2.weight', 'module.text_encoder.transformer.resblocks.5.ln_2.bias', 'module.text_encoder.transformer.resblocks.6.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.6.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.6.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.6.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.6.ln_1.weight', 'module.text_encoder.transformer.resblocks.6.ln_1.bias', 'module.text_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.6.ln_2.weight', 'module.text_encoder.transformer.resblocks.6.ln_2.bias', 'module.text_encoder.transformer.resblocks.7.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.7.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.7.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.7.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.7.ln_1.weight', 'module.text_encoder.transformer.resblocks.7.ln_1.bias', 'module.text_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.7.ln_2.weight', 'module.text_encoder.transformer.resblocks.7.ln_2.bias', 'module.text_encoder.transformer.resblocks.8.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.8.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.8.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.8.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.8.ln_1.weight', 'module.text_encoder.transformer.resblocks.8.ln_1.bias', 'module.text_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.8.ln_2.weight', 'module.text_encoder.transformer.resblocks.8.ln_2.bias', 'module.text_encoder.transformer.resblocks.9.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.9.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.9.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.9.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.9.ln_1.weight', 'module.text_encoder.transformer.resblocks.9.ln_1.bias', 'module.text_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.9.ln_2.weight', 'module.text_encoder.transformer.resblocks.9.ln_2.bias', 'module.text_encoder.transformer.resblocks.10.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.10.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.10.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.10.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.10.ln_1.weight', 'module.text_encoder.transformer.resblocks.10.ln_1.bias', 'module.text_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.10.ln_2.weight', 'module.text_encoder.transformer.resblocks.10.ln_2.bias', 'module.text_encoder.transformer.resblocks.11.attn.in_proj_weight', 'module.text_encoder.transformer.resblocks.11.attn.in_proj_bias', 'module.text_encoder.transformer.resblocks.11.attn.out_proj.weight', 'module.text_encoder.transformer.resblocks.11.attn.out_proj.bias', 'module.text_encoder.transformer.resblocks.11.ln_1.weight', 'module.text_encoder.transformer.resblocks.11.ln_1.bias', 'module.text_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'module.text_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'module.text_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'module.text_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'module.text_encoder.transformer.resblocks.11.ln_2.weight', 'module.text_encoder.transformer.resblocks.11.ln_2.bias', 'module.text_encoder.ln_final.weight', 'module.text_encoder.ln_final.bias'], unexpected_keys=[])
[2024-12-24 21:40:19 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][0/11317]	eta 17:09:07 lr 0.000022000	time 5.4561 (5.4561)	dist_loss 0.4401 (0.4401)	image_text_loss 3.8434 (3.8434)	pose_text_loss 3.1541 (3.1541)	tot_loss 11.3984 (11.3984)	mem 29650MB
[2024-12-24 21:40:37 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][50/11317]	eta 1:26:18 lr 0.000021999	time 0.3768 (0.4596)	dist_loss 0.0053 (0.1055)	image_text_loss 3.8882 (3.8754)	pose_text_loss 3.8573 (3.7157)	tot_loss 7.7982 (8.6464)	mem 29650MB
[2024-12-24 21:40:56 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][100/11317]	eta 1:17:40 lr 0.000021996	time 0.3824 (0.4155)	dist_loss 0.0143 (0.0609)	image_text_loss 3.7726 (3.8737)	pose_text_loss 3.6999 (3.7342)	tot_loss 7.6156 (8.2166)	mem 29650MB
[2024-12-24 21:41:14 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][150/11317]	eta 1:14:18 lr 0.000021991	time 0.3792 (0.3993)	dist_loss 0.0122 (0.0462)	image_text_loss 3.8051 (3.8723)	pose_text_loss 3.5601 (3.7174)	tot_loss 7.4877 (8.0516)	mem 29650MB
[2024-12-24 21:41:32 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][200/11317]	eta 1:12:19 lr 0.000021983	time 0.3610 (0.3904)	dist_loss 0.0345 (0.0390)	image_text_loss 3.9625 (3.8702)	pose_text_loss 3.6389 (3.6997)	tot_loss 7.9466 (7.9597)	mem 29650MB
[2024-12-24 21:41:50 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][250/11317]	eta 1:10:41 lr 0.000021974	time 0.3544 (0.3833)	dist_loss 0.0167 (0.0349)	image_text_loss 4.0164 (3.8741)	pose_text_loss 3.9266 (3.6764)	tot_loss 8.1099 (7.8989)	mem 29650MB
[2024-12-24 21:42:08 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][300/11317]	eta 1:09:33 lr 0.000021962	time 0.3605 (0.3788)	dist_loss 0.0308 (0.0323)	image_text_loss 3.7233 (3.8729)	pose_text_loss 3.4428 (3.6625)	tot_loss 7.4741 (7.8583)	mem 29650MB
[2024-12-24 21:42:26 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][350/11317]	eta 1:08:42 lr 0.000021949	time 0.3673 (0.3759)	dist_loss 0.0105 (0.0305)	image_text_loss 3.7644 (3.8723)	pose_text_loss 3.5073 (3.6457)	tot_loss 7.3763 (7.8225)	mem 29650MB
[2024-12-24 21:42:44 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][400/11317]	eta 1:08:00 lr 0.000021933	time 0.3453 (0.3738)	dist_loss 0.0199 (0.0291)	image_text_loss 3.8856 (3.8741)	pose_text_loss 3.3833 (3.6302)	tot_loss 7.4678 (7.7953)	mem 29650MB
[2024-12-24 21:43:01 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][450/11317]	eta 1:07:15 lr 0.000021915	time 0.3520 (0.3714)	dist_loss 0.0153 (0.0282)	image_text_loss 3.8973 (3.8740)	pose_text_loss 3.7502 (3.6162)	tot_loss 7.8002 (7.7725)	mem 29650MB
[2024-12-24 21:43:19 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][500/11317]	eta 1:06:42 lr 0.000021895	time 0.3503 (0.3700)	dist_loss 0.0151 (0.0275)	image_text_loss 3.8225 (3.8750)	pose_text_loss 3.5668 (3.6073)	tot_loss 7.5406 (7.7570)	mem 29650MB
[2024-12-24 21:43:37 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][550/11317]	eta 1:06:02 lr 0.000021873	time 0.3510 (0.3680)	dist_loss 0.0088 (0.0268)	image_text_loss 3.7921 (3.8757)	pose_text_loss 3.5432 (3.6002)	tot_loss 7.4230 (7.7438)	mem 29650MB
[2024-12-24 21:43:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][600/11317]	eta 1:05:27 lr 0.000021849	time 0.3555 (0.3665)	dist_loss 0.0204 (0.0264)	image_text_loss 3.8219 (3.8761)	pose_text_loss 3.5414 (3.5901)	tot_loss 7.5673 (7.7305)	mem 29650MB
[2024-12-24 21:44:12 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][650/11317]	eta 1:05:02 lr 0.000021823	time 0.3667 (0.3658)	dist_loss 0.0090 (0.0260)	image_text_loss 3.8772 (3.8758)	pose_text_loss 3.6462 (3.5851)	tot_loss 7.6132 (7.7211)	mem 29650MB
[2024-12-24 21:44:30 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][700/11317]	eta 1:04:34 lr 0.000021795	time 0.3322 (0.3650)	dist_loss 0.0230 (0.0259)	image_text_loss 3.8376 (3.8759)	pose_text_loss 3.4905 (3.5743)	tot_loss 7.5582 (7.7095)	mem 29650MB
[2024-12-24 21:44:47 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][750/11317]	eta 1:04:06 lr 0.000021765	time 0.3552 (0.3640)	dist_loss 0.0162 (0.0256)	image_text_loss 3.8997 (3.8760)	pose_text_loss 3.6485 (3.5688)	tot_loss 7.7100 (7.7005)	mem 29650MB
[2024-12-24 21:45:05 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][800/11317]	eta 1:03:46 lr 0.000021733	time 0.3523 (0.3638)	dist_loss 0.0167 (0.0255)	image_text_loss 3.8729 (3.8757)	pose_text_loss 3.4215 (3.5573)	tot_loss 7.4613 (7.6879)	mem 29650MB
[2024-12-24 21:45:23 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][850/11317]	eta 1:03:23 lr 0.000021698	time 0.3499 (0.3633)	dist_loss 0.0274 (0.0257)	image_text_loss 3.9372 (3.8756)	pose_text_loss 3.5790 (3.5453)	tot_loss 7.7902 (7.6774)	mem 29650MB
[2024-12-24 21:45:41 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][900/11317]	eta 1:02:59 lr 0.000021662	time 0.4466 (0.3629)	dist_loss 0.0229 (0.0256)	image_text_loss 3.8981 (3.8755)	pose_text_loss 3.1734 (3.5369)	tot_loss 7.3006 (7.6685)	mem 29650MB
[2024-12-24 21:45:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][950/11317]	eta 1:02:37 lr 0.000021624	time 0.3479 (0.3624)	dist_loss 0.0339 (0.0256)	image_text_loss 3.7481 (3.8756)	pose_text_loss 3.0080 (3.5304)	tot_loss 7.0947 (7.6618)	mem 29650MB
[2024-12-24 21:46:16 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1000/11317]	eta 1:02:14 lr 0.000021583	time 0.3570 (0.3620)	dist_loss 0.0322 (0.0255)	image_text_loss 3.8654 (3.8756)	pose_text_loss 3.4219 (3.5230)	tot_loss 7.6098 (7.6539)	mem 29650MB
[2024-12-24 21:46:34 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1050/11317]	eta 1:01:51 lr 0.000021541	time 0.3473 (0.3615)	dist_loss 0.0359 (0.0256)	image_text_loss 3.7928 (3.8748)	pose_text_loss 2.9734 (3.5136)	tot_loss 7.1256 (7.6444)	mem 29650MB
[2024-12-24 21:46:52 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1100/11317]	eta 1:01:32 lr 0.000021496	time 0.3569 (0.3614)	dist_loss 0.0324 (0.0258)	image_text_loss 3.8818 (3.8757)	pose_text_loss 3.4958 (3.5031)	tot_loss 7.7012 (7.6366)	mem 29650MB
[2024-12-24 21:47:09 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1150/11317]	eta 1:01:12 lr 0.000021450	time 0.3550 (0.3612)	dist_loss 0.0272 (0.0258)	image_text_loss 3.9669 (3.8757)	pose_text_loss 3.6694 (3.4992)	tot_loss 7.9086 (7.6331)	mem 29650MB
[2024-12-24 21:47:27 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1200/11317]	eta 1:00:52 lr 0.000021401	time 0.3509 (0.3610)	dist_loss 0.0363 (0.0261)	image_text_loss 3.9279 (3.8756)	pose_text_loss 3.2335 (3.4873)	tot_loss 7.5246 (7.6236)	mem 29650MB
[2024-12-24 21:47:45 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1250/11317]	eta 1:00:32 lr 0.000021351	time 0.3423 (0.3608)	dist_loss 0.0661 (0.0263)	image_text_loss 3.9801 (3.8753)	pose_text_loss 2.6091 (3.4783)	tot_loss 7.2505 (7.6167)	mem 29650MB
[2024-12-24 21:48:03 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1300/11317]	eta 1:00:10 lr 0.000021299	time 0.3441 (0.3605)	dist_loss 0.0277 (0.0263)	image_text_loss 3.9335 (3.8750)	pose_text_loss 3.6732 (3.4725)	tot_loss 7.8838 (7.6107)	mem 29650MB
[2024-12-24 21:48:21 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1350/11317]	eta 0:59:52 lr 0.000021244	time 0.3498 (0.3604)	dist_loss 0.0881 (0.0265)	image_text_loss 3.9444 (3.8752)	pose_text_loss 2.5598 (3.4646)	tot_loss 7.3848 (7.6052)	mem 29650MB
[2024-12-24 21:48:39 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1400/11317]	eta 0:59:33 lr 0.000021188	time 0.3533 (0.3603)	dist_loss 0.0256 (0.0267)	image_text_loss 3.8432 (3.8752)	pose_text_loss 3.4777 (3.4559)	tot_loss 7.5767 (7.5982)	mem 29650MB
[2024-12-24 21:48:56 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1450/11317]	eta 0:59:12 lr 0.000021130	time 0.3444 (0.3600)	dist_loss 0.0393 (0.0268)	image_text_loss 3.9711 (3.8756)	pose_text_loss 3.2754 (3.4490)	tot_loss 7.6398 (7.5930)	mem 29650MB
[2024-12-24 21:49:14 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1500/11317]	eta 0:58:51 lr 0.000021069	time 0.4277 (0.3598)	dist_loss 0.0112 (0.0271)	image_text_loss 3.8908 (3.8757)	pose_text_loss 3.8909 (3.4396)	tot_loss 7.8932 (7.5864)	mem 29650MB
[2024-12-24 21:49:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1550/11317]	eta 0:58:31 lr 0.000021007	time 0.3589 (0.3596)	dist_loss 0.0105 (0.0274)	image_text_loss 3.8538 (3.8755)	pose_text_loss 3.5433 (3.4307)	tot_loss 7.5025 (7.5798)	mem 29650MB
[2024-12-24 21:49:49 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1600/11317]	eta 0:58:12 lr 0.000020943	time 0.3446 (0.3594)	dist_loss 0.0352 (0.0275)	image_text_loss 3.8213 (3.8755)	pose_text_loss 3.2540 (3.4241)	tot_loss 7.4269 (7.5747)	mem 29650MB
[2024-12-24 21:50:07 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1650/11317]	eta 0:57:54 lr 0.000020877	time 0.3510 (0.3594)	dist_loss 0.0216 (0.0276)	image_text_loss 3.8230 (3.8754)	pose_text_loss 3.1681 (3.4193)	tot_loss 7.2071 (7.5709)	mem 29650MB
[2024-12-24 21:50:25 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1700/11317]	eta 0:57:35 lr 0.000020810	time 0.3468 (0.3593)	dist_loss 0.0324 (0.0277)	image_text_loss 3.8623 (3.8747)	pose_text_loss 3.0460 (3.4149)	tot_loss 7.2327 (7.5662)	mem 29650MB
[2024-12-24 21:50:43 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1750/11317]	eta 0:57:15 lr 0.000020740	time 0.3480 (0.3591)	dist_loss 0.0157 (0.0278)	image_text_loss 3.8674 (3.8747)	pose_text_loss 3.4639 (3.4085)	tot_loss 7.4884 (7.5612)	mem 29650MB
[2024-12-24 21:51:00 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1800/11317]	eta 0:56:57 lr 0.000020669	time 0.4344 (0.3591)	dist_loss 0.0624 (0.0280)	image_text_loss 3.8673 (3.8747)	pose_text_loss 2.5478 (3.4016)	tot_loss 7.0392 (7.5559)	mem 29650MB
[2024-12-24 21:51:18 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1850/11317]	eta 0:56:37 lr 0.000020595	time 0.3590 (0.3589)	dist_loss 0.0345 (0.0282)	image_text_loss 3.8703 (3.8747)	pose_text_loss 3.2990 (3.3950)	tot_loss 7.5144 (7.5514)	mem 29650MB
[2024-12-24 21:51:36 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1900/11317]	eta 0:56:18 lr 0.000020520	time 0.3635 (0.3588)	dist_loss 0.0258 (0.0284)	image_text_loss 3.8724 (3.8746)	pose_text_loss 3.5137 (3.3871)	tot_loss 7.6445 (7.5459)	mem 29650MB
[2024-12-24 21:51:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][1950/11317]	eta 0:56:00 lr 0.000020443	time 0.3663 (0.3588)	dist_loss 0.0472 (0.0288)	image_text_loss 3.8651 (3.8745)	pose_text_loss 2.8278 (3.3770)	tot_loss 7.1648 (7.5392)	mem 29650MB
[2024-12-24 21:52:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2000/11317]	eta 0:55:41 lr 0.000020364	time 0.3492 (0.3586)	dist_loss 0.0152 (0.0291)	image_text_loss 3.8693 (3.8743)	pose_text_loss 3.4056 (3.3673)	tot_loss 7.4264 (7.5327)	mem 29650MB
[2024-12-24 21:52:29 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2050/11317]	eta 0:55:22 lr 0.000020284	time 0.3575 (0.3585)	dist_loss 0.0389 (0.0293)	image_text_loss 3.9248 (3.8742)	pose_text_loss 3.0183 (3.3608)	tot_loss 7.3319 (7.5280)	mem 29650MB
[2024-12-24 21:52:47 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2100/11317]	eta 0:55:03 lr 0.000020201	time 0.3543 (0.3584)	dist_loss 0.0462 (0.0296)	image_text_loss 3.8591 (3.8741)	pose_text_loss 2.8455 (3.3526)	tot_loss 7.1671 (7.5222)	mem 29650MB
[2024-12-24 21:53:05 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2150/11317]	eta 0:54:44 lr 0.000020117	time 0.3559 (0.3583)	dist_loss 0.0394 (0.0297)	image_text_loss 3.8723 (3.8740)	pose_text_loss 2.9035 (3.3472)	tot_loss 7.1693 (7.5182)	mem 29650MB
[2024-12-24 21:53:22 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2200/11317]	eta 0:54:26 lr 0.000020031	time 0.3690 (0.3583)	dist_loss 0.0229 (0.0299)	image_text_loss 3.8720 (3.8740)	pose_text_loss 3.6066 (3.3413)	tot_loss 7.7081 (7.5144)	mem 29650MB
[2024-12-24 21:53:40 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2250/11317]	eta 0:54:07 lr 0.000019944	time 0.3527 (0.3581)	dist_loss 0.0585 (0.0301)	image_text_loss 3.8771 (3.8739)	pose_text_loss 2.5544 (3.3336)	tot_loss 7.0168 (7.5088)	mem 29650MB
[2024-12-24 21:53:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2300/11317]	eta 0:53:48 lr 0.000019855	time 0.3514 (0.3581)	dist_loss 0.0325 (0.0303)	image_text_loss 3.8808 (3.8738)	pose_text_loss 3.3084 (3.3275)	tot_loss 7.5143 (7.5047)	mem 29650MB
[2024-12-24 21:54:15 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2350/11317]	eta 0:53:29 lr 0.000019764	time 0.3451 (0.3579)	dist_loss 0.0530 (0.0305)	image_text_loss 3.9081 (3.8733)	pose_text_loss 2.6625 (3.3213)	tot_loss 7.1008 (7.4998)	mem 29650MB
[2024-12-24 21:54:33 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2400/11317]	eta 0:53:11 lr 0.000019671	time 0.3639 (0.3579)	dist_loss 0.0094 (0.0307)	image_text_loss 3.8823 (3.8734)	pose_text_loss 3.8620 (3.3173)	tot_loss 7.8382 (7.4974)	mem 29650MB
[2024-12-24 21:54:51 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2450/11317]	eta 0:52:52 lr 0.000019577	time 0.3490 (0.3578)	dist_loss 0.0342 (0.0309)	image_text_loss 3.8677 (3.8732)	pose_text_loss 3.0629 (3.3113)	tot_loss 7.2730 (7.4931)	mem 29650MB
[2024-12-24 21:55:08 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2500/11317]	eta 0:52:33 lr 0.000019481	time 0.3362 (0.3577)	dist_loss 0.0632 (0.0310)	image_text_loss 3.8654 (3.8733)	pose_text_loss 2.5780 (3.3057)	tot_loss 7.0751 (7.4894)	mem 29650MB
[2024-12-24 21:55:26 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2550/11317]	eta 0:52:15 lr 0.000019384	time 0.3562 (0.3576)	dist_loss 0.0312 (0.0313)	image_text_loss 3.8719 (3.8733)	pose_text_loss 3.2858 (3.2987)	tot_loss 7.4694 (7.4851)	mem 29650MB
[2024-12-24 21:55:44 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2600/11317]	eta 0:51:57 lr 0.000019285	time 0.3399 (0.3576)	dist_loss 0.0212 (0.0315)	image_text_loss 3.8585 (3.8732)	pose_text_loss 3.4912 (3.2926)	tot_loss 7.5616 (7.4808)	mem 29650MB
[2024-12-24 21:56:01 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2650/11317]	eta 0:51:38 lr 0.000019184	time 0.3442 (0.3575)	dist_loss 0.0248 (0.0317)	image_text_loss 3.8612 (3.8733)	pose_text_loss 3.6096 (3.2873)	tot_loss 7.7186 (7.4774)	mem 29650MB
[2024-12-24 21:56:19 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2700/11317]	eta 0:51:20 lr 0.000019082	time 0.3559 (0.3575)	dist_loss 0.0293 (0.0318)	image_text_loss 3.8811 (3.8733)	pose_text_loss 3.3725 (3.2820)	tot_loss 7.5471 (7.4738)	mem 29650MB
[2024-12-24 21:56:37 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2750/11317]	eta 0:51:02 lr 0.000018978	time 0.3465 (0.3574)	dist_loss 0.0474 (0.0320)	image_text_loss 3.8687 (3.8733)	pose_text_loss 2.7130 (3.2766)	tot_loss 7.0557 (7.4703)	mem 29650MB
[2024-12-24 21:56:55 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2800/11317]	eta 0:50:43 lr 0.000018873	time 0.3771 (0.3574)	dist_loss 0.0357 (0.0322)	image_text_loss 3.7862 (3.8732)	pose_text_loss 2.7361 (3.2718)	tot_loss 6.8788 (7.4667)	mem 29650MB
[2024-12-24 21:57:13 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2850/11317]	eta 0:50:26 lr 0.000018766	time 0.3538 (0.3574)	dist_loss 0.0306 (0.0323)	image_text_loss 3.8823 (3.8733)	pose_text_loss 3.1535 (3.2678)	tot_loss 7.3420 (7.4642)	mem 29650MB
[2024-12-24 21:57:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2900/11317]	eta 0:50:08 lr 0.000018658	time 0.3530 (0.3574)	dist_loss 0.0150 (0.0325)	image_text_loss 3.7991 (3.8732)	pose_text_loss 3.3592 (3.2632)	tot_loss 7.3083 (7.4609)	mem 29650MB
[2024-12-24 21:57:48 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][2950/11317]	eta 0:49:50 lr 0.000018548	time 0.3625 (0.3574)	dist_loss 0.0647 (0.0326)	image_text_loss 3.8713 (3.8732)	pose_text_loss 2.4934 (3.2586)	tot_loss 7.0122 (7.4580)	mem 29650MB
[2024-12-24 21:58:06 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3000/11317]	eta 0:49:32 lr 0.000018437	time 0.3609 (0.3574)	dist_loss 0.0355 (0.0328)	image_text_loss 3.8666 (3.8731)	pose_text_loss 3.0751 (3.2539)	tot_loss 7.2966 (7.4546)	mem 29650MB
[2024-12-24 21:58:24 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3050/11317]	eta 0:49:14 lr 0.000018324	time 0.3552 (0.3574)	dist_loss 0.0400 (0.0329)	image_text_loss 3.8715 (3.8731)	pose_text_loss 2.8310 (3.2496)	tot_loss 7.1026 (7.4517)	mem 29650MB
[2024-12-24 21:58:42 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3100/11317]	eta 0:48:56 lr 0.000018210	time 0.3538 (0.3573)	dist_loss 0.0367 (0.0330)	image_text_loss 3.8711 (3.8730)	pose_text_loss 2.8631 (3.2449)	tot_loss 7.1011 (7.4484)	mem 29650MB
[2024-12-24 21:59:00 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3150/11317]	eta 0:48:38 lr 0.000018095	time 0.3596 (0.3573)	dist_loss 0.0265 (0.0332)	image_text_loss 3.8684 (3.8730)	pose_text_loss 3.3615 (3.2398)	tot_loss 7.4945 (7.4450)	mem 29650MB
[2024-12-24 21:59:17 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3200/11317]	eta 0:48:20 lr 0.000017979	time 0.3546 (0.3573)	dist_loss 0.0363 (0.0334)	image_text_loss 3.8701 (3.8730)	pose_text_loss 2.8829 (3.2348)	tot_loss 7.1156 (7.4418)	mem 29650MB
[2024-12-24 21:59:35 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3250/11317]	eta 0:48:01 lr 0.000017861	time 0.3631 (0.3572)	dist_loss 0.0382 (0.0335)	image_text_loss 3.8712 (3.8730)	pose_text_loss 2.8007 (3.2307)	tot_loss 7.0541 (7.4389)	mem 29650MB
[2024-12-24 21:59:53 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3300/11317]	eta 0:47:44 lr 0.000017741	time 0.3551 (0.3572)	dist_loss 0.0607 (0.0337)	image_text_loss 3.8724 (3.8730)	pose_text_loss 2.4781 (3.2264)	tot_loss 6.9572 (7.4360)	mem 29650MB
[2024-12-24 22:00:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3350/11317]	eta 0:47:25 lr 0.000017621	time 0.3534 (0.3572)	dist_loss 0.0437 (0.0338)	image_text_loss 3.8722 (3.8729)	pose_text_loss 2.7641 (3.2216)	tot_loss 7.0730 (7.4328)	mem 29650MB
[2024-12-24 22:00:28 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3400/11317]	eta 0:47:07 lr 0.000017499	time 0.3449 (0.3571)	dist_loss 0.0190 (0.0340)	image_text_loss 3.8722 (3.8729)	pose_text_loss 3.9628 (3.2180)	tot_loss 8.0254 (7.4307)	mem 29650MB
[2024-12-24 22:00:46 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3450/11317]	eta 0:46:49 lr 0.000017376	time 0.3444 (0.3572)	dist_loss 0.0144 (0.0341)	image_text_loss 3.8708 (3.8729)	pose_text_loss 3.8380 (3.2136)	tot_loss 7.8527 (7.4279)	mem 29650MB
[2024-12-24 22:01:04 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3500/11317]	eta 0:46:31 lr 0.000017252	time 0.3520 (0.3571)	dist_loss 0.0604 (0.0343)	image_text_loss 3.8992 (3.8729)	pose_text_loss 2.5491 (3.2099)	tot_loss 7.0527 (7.4255)	mem 29650MB
[2024-12-24 22:01:22 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3550/11317]	eta 0:46:13 lr 0.000017126	time 0.3675 (0.3571)	dist_loss 0.0505 (0.0344)	image_text_loss 3.8700 (3.8728)	pose_text_loss 2.5999 (3.2063)	tot_loss 6.9744 (7.4228)	mem 29650MB
[2024-12-24 22:01:40 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3600/11317]	eta 0:45:55 lr 0.000017000	time 0.3555 (0.3571)	dist_loss 0.0381 (0.0345)	image_text_loss 3.8723 (3.8728)	pose_text_loss 2.8182 (3.2024)	tot_loss 7.0717 (7.4201)	mem 29650MB
[2024-12-24 22:01:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3650/11317]	eta 0:45:37 lr 0.000016872	time 0.3475 (0.3571)	dist_loss 0.0417 (0.0346)	image_text_loss 3.8658 (3.8728)	pose_text_loss 2.7727 (3.1985)	tot_loss 7.0551 (7.4177)	mem 29650MB
[2024-12-24 22:02:15 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3700/11317]	eta 0:45:19 lr 0.000016743	time 0.3545 (0.3570)	dist_loss 0.0258 (0.0348)	image_text_loss 3.8742 (3.8728)	pose_text_loss 3.1724 (3.1944)	tot_loss 7.3042 (7.4147)	mem 29650MB
[2024-12-24 22:02:33 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3750/11317]	eta 0:45:01 lr 0.000016613	time 0.3448 (0.3571)	dist_loss 0.0334 (0.0349)	image_text_loss 3.8721 (3.8728)	pose_text_loss 3.1822 (3.1909)	tot_loss 7.3882 (7.4126)	mem 29650MB
[2024-12-24 22:02:51 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3800/11317]	eta 0:44:43 lr 0.000016482	time 0.3580 (0.3570)	dist_loss 0.0605 (0.0351)	image_text_loss 3.8702 (3.8728)	pose_text_loss 2.5704 (3.1871)	tot_loss 7.0461 (7.4104)	mem 29650MB
[2024-12-24 22:03:09 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3850/11317]	eta 0:44:25 lr 0.000016350	time 0.3602 (0.3570)	dist_loss 0.0541 (0.0352)	image_text_loss 3.8726 (3.8728)	pose_text_loss 2.5476 (3.1832)	tot_loss 6.9615 (7.4077)	mem 29650MB
[2024-12-24 22:03:27 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3900/11317]	eta 0:44:08 lr 0.000016217	time 0.3406 (0.3571)	dist_loss 0.0138 (0.0353)	image_text_loss 3.8748 (3.8727)	pose_text_loss 3.5205 (3.1788)	tot_loss 7.5328 (7.4046)	mem 29650MB
[2024-12-24 22:03:45 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][3950/11317]	eta 0:43:50 lr 0.000016083	time 0.3459 (0.3571)	dist_loss 0.0366 (0.0354)	image_text_loss 3.8714 (3.8727)	pose_text_loss 3.0867 (3.1755)	tot_loss 7.3242 (7.4024)	mem 29650MB
[2024-12-24 22:04:02 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4000/11317]	eta 0:43:32 lr 0.000015948	time 0.3526 (0.3570)	dist_loss 0.0286 (0.0355)	image_text_loss 3.8751 (3.8727)	pose_text_loss 3.2721 (3.1721)	tot_loss 7.4335 (7.4001)	mem 29650MB
[2024-12-24 22:04:20 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4050/11317]	eta 0:43:14 lr 0.000015813	time 0.3558 (0.3570)	dist_loss 0.0525 (0.0356)	image_text_loss 3.8680 (3.8727)	pose_text_loss 2.6223 (3.1693)	tot_loss 7.0158 (7.3982)	mem 29650MB
[2024-12-24 22:04:38 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4100/11317]	eta 0:42:56 lr 0.000015676	time 0.3441 (0.3570)	dist_loss 0.0404 (0.0358)	image_text_loss 3.8718 (3.8727)	pose_text_loss 3.1352 (3.1657)	tot_loss 7.4109 (7.3959)	mem 29650MB
[2024-12-24 22:04:56 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4150/11317]	eta 0:42:38 lr 0.000015538	time 0.3531 (0.3570)	dist_loss 0.0213 (0.0358)	image_text_loss 3.8714 (3.8727)	pose_text_loss 3.5887 (3.1629)	tot_loss 7.6728 (7.3940)	mem 29650MB
[2024-12-24 22:05:13 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4200/11317]	eta 0:42:20 lr 0.000015400	time 0.3449 (0.3569)	dist_loss 0.0423 (0.0359)	image_text_loss 3.8728 (3.8727)	pose_text_loss 3.5908 (3.1615)	tot_loss 7.8865 (7.3929)	mem 29650MB
[2024-12-24 22:05:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4250/11317]	eta 0:42:02 lr 0.000015260	time 0.3437 (0.3569)	dist_loss 0.0615 (0.0360)	image_text_loss 3.8770 (3.8727)	pose_text_loss 2.5477 (3.1580)	tot_loss 7.0401 (7.3906)	mem 29650MB
[2024-12-24 22:05:49 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4300/11317]	eta 0:41:44 lr 0.000015120	time 0.3540 (0.3569)	dist_loss 0.0280 (0.0361)	image_text_loss 3.8762 (3.8727)	pose_text_loss 3.3312 (3.1551)	tot_loss 7.4876 (7.3887)	mem 29650MB
[2024-12-24 22:06:07 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4350/11317]	eta 0:41:26 lr 0.000014979	time 0.3692 (0.3569)	dist_loss 0.0321 (0.0362)	image_text_loss 3.8426 (3.8726)	pose_text_loss 2.9883 (3.1513)	tot_loss 7.1516 (7.3860)	mem 29650MB
[2024-12-24 22:06:25 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4400/11317]	eta 0:41:08 lr 0.000014838	time 0.3475 (0.3569)	dist_loss 0.0454 (0.0363)	image_text_loss 3.8873 (3.8726)	pose_text_loss 2.7055 (3.1486)	tot_loss 7.0465 (7.3842)	mem 29650MB
[2024-12-24 22:06:42 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4450/11317]	eta 0:40:50 lr 0.000014695	time 0.3440 (0.3569)	dist_loss 0.0428 (0.0364)	image_text_loss 3.8737 (3.8726)	pose_text_loss 2.8356 (3.1453)	tot_loss 7.1373 (7.3819)	mem 29650MB
[2024-12-24 22:07:00 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4500/11317]	eta 0:40:33 lr 0.000014552	time 0.3375 (0.3569)	dist_loss 0.0304 (0.0365)	image_text_loss 3.8650 (3.8726)	pose_text_loss 3.2615 (3.1416)	tot_loss 7.4304 (7.3795)	mem 29650MB
[2024-12-24 22:07:18 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4550/11317]	eta 0:40:15 lr 0.000014408	time 0.3554 (0.3569)	dist_loss 0.0353 (0.0367)	image_text_loss 3.8718 (3.8726)	pose_text_loss 2.9269 (3.1368)	tot_loss 7.1516 (7.3763)	mem 29650MB
[2024-12-24 22:07:36 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4600/11317]	eta 0:39:56 lr 0.000014264	time 0.3664 (0.3568)	dist_loss 0.0917 (0.0368)	image_text_loss 3.8743 (3.8726)	pose_text_loss 2.1975 (3.1342)	tot_loss 6.9891 (7.3745)	mem 29650MB
[2024-12-24 22:07:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4650/11317]	eta 0:39:39 lr 0.000014119	time 0.4457 (0.3569)	dist_loss 0.0614 (0.0369)	image_text_loss 3.8678 (3.8726)	pose_text_loss 2.5015 (3.1324)	tot_loss 6.9832 (7.3736)	mem 29650MB
[2024-12-24 22:08:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4700/11317]	eta 0:39:21 lr 0.000013974	time 0.3515 (0.3569)	dist_loss 0.0337 (0.0370)	image_text_loss 3.8731 (3.8726)	pose_text_loss 3.0418 (3.1283)	tot_loss 7.2523 (7.3709)	mem 29650MB
[2024-12-24 22:08:29 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4750/11317]	eta 0:39:03 lr 0.000013827	time 0.3606 (0.3568)	dist_loss 0.0647 (0.0370)	image_text_loss 3.8642 (3.8727)	pose_text_loss 2.4098 (3.1268)	tot_loss 6.9210 (7.3699)	mem 29650MB
[2024-12-24 22:08:47 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4800/11317]	eta 0:38:45 lr 0.000013681	time 0.3500 (0.3569)	dist_loss 0.0448 (0.0372)	image_text_loss 3.8761 (3.8726)	pose_text_loss 2.7797 (3.1237)	tot_loss 7.1038 (7.3679)	mem 29650MB
[2024-12-24 22:09:05 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4850/11317]	eta 0:38:27 lr 0.000013534	time 0.3546 (0.3568)	dist_loss 0.0671 (0.0372)	image_text_loss 3.7865 (3.8727)	pose_text_loss 2.2940 (3.1214)	tot_loss 6.7515 (7.3662)	mem 29650MB
[2024-12-24 22:09:22 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4900/11317]	eta 0:38:09 lr 0.000013386	time 0.3515 (0.3568)	dist_loss 0.0767 (0.0373)	image_text_loss 3.8735 (3.8726)	pose_text_loss 2.2787 (3.1190)	tot_loss 6.9192 (7.3645)	mem 29650MB
[2024-12-24 22:09:40 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][4950/11317]	eta 0:37:51 lr 0.000013238	time 0.3487 (0.3568)	dist_loss 0.0346 (0.0374)	image_text_loss 3.8632 (3.8726)	pose_text_loss 3.1247 (3.1162)	tot_loss 7.3339 (7.3628)	mem 29650MB
[2024-12-24 22:09:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5000/11317]	eta 0:37:33 lr 0.000013090	time 0.3553 (0.3568)	dist_loss 0.0544 (0.0375)	image_text_loss 3.8174 (3.8726)	pose_text_loss 2.6409 (3.1135)	tot_loss 7.0023 (7.3610)	mem 29650MB
[2024-12-24 22:10:16 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5050/11317]	eta 0:37:16 lr 0.000012941	time 0.3612 (0.3568)	dist_loss 0.0390 (0.0376)	image_text_loss 3.8725 (3.8726)	pose_text_loss 2.8194 (3.1108)	tot_loss 7.0821 (7.3591)	mem 29650MB
[2024-12-24 22:10:34 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5100/11317]	eta 0:36:58 lr 0.000012792	time 0.3481 (0.3568)	dist_loss 0.0258 (0.0376)	image_text_loss 3.8703 (3.8726)	pose_text_loss 3.5608 (3.1083)	tot_loss 7.6895 (7.3574)	mem 29650MB
[2024-12-24 22:10:52 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5150/11317]	eta 0:36:40 lr 0.000012642	time 0.3631 (0.3568)	dist_loss 0.0813 (0.0377)	image_text_loss 3.8728 (3.8726)	pose_text_loss 2.2389 (3.1068)	tot_loss 6.9250 (7.3563)	mem 29650MB
[2024-12-24 22:11:10 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5200/11317]	eta 0:36:22 lr 0.000012492	time 0.3551 (0.3568)	dist_loss 0.0376 (0.0378)	image_text_loss 3.8868 (3.8725)	pose_text_loss 3.2080 (3.1039)	tot_loss 7.4705 (7.3543)	mem 29650MB
[2024-12-24 22:11:28 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5250/11317]	eta 0:36:05 lr 0.000012342	time 0.3561 (0.3569)	dist_loss 0.0824 (0.0379)	image_text_loss 3.8730 (3.8725)	pose_text_loss 2.2371 (3.1014)	tot_loss 6.9338 (7.3527)	mem 29650MB
[2024-12-24 22:11:46 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5300/11317]	eta 0:35:47 lr 0.000012192	time 0.3569 (0.3569)	dist_loss 0.0174 (0.0380)	image_text_loss 3.8744 (3.8725)	pose_text_loss 3.5657 (3.0987)	tot_loss 7.6139 (7.3510)	mem 29650MB
[2024-12-24 22:12:03 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5350/11317]	eta 0:35:29 lr 0.000012041	time 0.3634 (0.3569)	dist_loss 0.0634 (0.0381)	image_text_loss 3.8705 (3.8725)	pose_text_loss 2.4410 (3.0961)	tot_loss 6.9455 (7.3493)	mem 29650MB
[2024-12-24 22:12:21 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5400/11317]	eta 0:35:11 lr 0.000011891	time 0.3537 (0.3569)	dist_loss 0.0505 (0.0381)	image_text_loss 3.8698 (3.8725)	pose_text_loss 2.6374 (3.0943)	tot_loss 7.0124 (7.3482)	mem 29650MB
[2024-12-24 22:12:39 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5450/11317]	eta 0:34:53 lr 0.000011740	time 0.3583 (0.3569)	dist_loss 0.0626 (0.0382)	image_text_loss 3.8755 (3.8725)	pose_text_loss 2.4979 (3.0918)	tot_loss 6.9998 (7.3466)	mem 29650MB
[2024-12-24 22:12:57 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5500/11317]	eta 0:34:36 lr 0.000011589	time 0.3597 (0.3569)	dist_loss 0.0350 (0.0383)	image_text_loss 3.8789 (3.8725)	pose_text_loss 3.0064 (3.0902)	tot_loss 7.2352 (7.3456)	mem 29650MB
[2024-12-24 22:13:15 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5550/11317]	eta 0:34:18 lr 0.000011438	time 0.3534 (0.3569)	dist_loss 0.0357 (0.0384)	image_text_loss 3.8846 (3.8725)	pose_text_loss 3.0314 (3.0884)	tot_loss 7.2729 (7.3444)	mem 29650MB
[2024-12-24 22:13:33 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5600/11317]	eta 0:34:00 lr 0.000011287	time 0.3637 (0.3569)	dist_loss 0.0121 (0.0384)	image_text_loss 3.8752 (3.8725)	pose_text_loss 3.9272 (3.0866)	tot_loss 7.9236 (7.3434)	mem 29650MB
[2024-12-24 22:13:51 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5650/11317]	eta 0:33:42 lr 0.000011136	time 0.3486 (0.3569)	dist_loss 0.0276 (0.0385)	image_text_loss 3.8777 (3.8725)	pose_text_loss 3.4186 (3.0847)	tot_loss 7.5728 (7.3420)	mem 29650MB
[2024-12-24 22:14:09 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5700/11317]	eta 0:33:24 lr 0.000010985	time 0.3542 (0.3569)	dist_loss 0.0482 (0.0386)	image_text_loss 3.8752 (3.8725)	pose_text_loss 2.6615 (3.0825)	tot_loss 7.0182 (7.3406)	mem 29650MB
[2024-12-24 22:14:26 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5750/11317]	eta 0:33:06 lr 0.000010833	time 0.3522 (0.3569)	dist_loss 0.0281 (0.0386)	image_text_loss 3.8724 (3.8724)	pose_text_loss 3.2400 (3.0804)	tot_loss 7.3938 (7.3391)	mem 29650MB
[2024-12-24 22:14:44 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5800/11317]	eta 0:32:49 lr 0.000010682	time 0.3468 (0.3569)	dist_loss 0.0575 (0.0387)	image_text_loss 3.8693 (3.8725)	pose_text_loss 2.4799 (3.0783)	tot_loss 6.9242 (7.3378)	mem 29650MB
[2024-12-24 22:15:02 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5850/11317]	eta 0:32:31 lr 0.000010531	time 0.3590 (0.3569)	dist_loss 0.0737 (0.0388)	image_text_loss 3.8718 (3.8724)	pose_text_loss 2.3313 (3.0764)	tot_loss 6.9404 (7.3365)	mem 29650MB
[2024-12-24 22:15:20 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5900/11317]	eta 0:32:13 lr 0.000010380	time 0.3705 (0.3569)	dist_loss 0.0376 (0.0389)	image_text_loss 3.8595 (3.8724)	pose_text_loss 2.8840 (3.0738)	tot_loss 7.1199 (7.3348)	mem 29650MB
[2024-12-24 22:15:37 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][5950/11317]	eta 0:31:55 lr 0.000010230	time 0.3629 (0.3569)	dist_loss 0.0644 (0.0389)	image_text_loss 3.8682 (3.8724)	pose_text_loss 2.4202 (3.0723)	tot_loss 6.9323 (7.3339)	mem 29650MB
[2024-12-24 22:15:56 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6000/11317]	eta 0:31:37 lr 0.000010079	time 0.3555 (0.3569)	dist_loss 0.0259 (0.0390)	image_text_loss 3.8612 (3.8724)	pose_text_loss 3.3580 (3.0704)	tot_loss 7.4784 (7.3327)	mem 29650MB
[2024-12-24 22:16:14 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6050/11317]	eta 0:31:19 lr 0.000009929	time 0.3485 (0.3569)	dist_loss 0.0663 (0.0391)	image_text_loss 3.8533 (3.8723)	pose_text_loss 2.4083 (3.0679)	tot_loss 6.9247 (7.3311)	mem 29650MB
[2024-12-24 22:16:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6100/11317]	eta 0:31:01 lr 0.000009779	time 0.3690 (0.3569)	dist_loss 0.0302 (0.0391)	image_text_loss 3.8699 (3.8723)	pose_text_loss 3.0921 (3.0662)	tot_loss 7.2645 (7.3300)	mem 29650MB
[2024-12-24 22:16:49 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6150/11317]	eta 0:30:44 lr 0.000009629	time 0.3587 (0.3569)	dist_loss 0.0307 (0.0392)	image_text_loss 3.8711 (3.8723)	pose_text_loss 3.3422 (3.0644)	tot_loss 7.5199 (7.3287)	mem 29650MB
[2024-12-24 22:17:07 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6200/11317]	eta 0:30:26 lr 0.000009479	time 0.3479 (0.3569)	dist_loss 0.0734 (0.0393)	image_text_loss 3.8795 (3.8723)	pose_text_loss 2.3401 (3.0627)	tot_loss 6.9538 (7.3276)	mem 29650MB
[2024-12-24 22:17:25 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6250/11317]	eta 0:30:08 lr 0.000009330	time 0.3723 (0.3569)	dist_loss 0.0490 (0.0393)	image_text_loss 3.8979 (3.8723)	pose_text_loss 2.7559 (3.0608)	tot_loss 7.1437 (7.3264)	mem 29650MB
[2024-12-24 22:17:43 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6300/11317]	eta 0:29:50 lr 0.000009181	time 0.3363 (0.3570)	dist_loss 0.0226 (0.0394)	image_text_loss 3.8927 (3.8723)	pose_text_loss 3.2875 (3.0588)	tot_loss 7.4062 (7.3250)	mem 29650MB
[2024-12-24 22:18:01 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6350/11317]	eta 0:29:32 lr 0.000009032	time 0.3614 (0.3569)	dist_loss 0.0700 (0.0395)	image_text_loss 3.8590 (3.8722)	pose_text_loss 2.3162 (3.0564)	tot_loss 6.8751 (7.3234)	mem 29650MB
[2024-12-24 22:18:18 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6400/11317]	eta 0:29:15 lr 0.000008884	time 0.3571 (0.3569)	dist_loss 0.0645 (0.0396)	image_text_loss 3.8701 (3.8722)	pose_text_loss 3.7296 (3.0543)	tot_loss 8.2446 (7.3220)	mem 29650MB
[2024-12-24 22:18:37 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6450/11317]	eta 0:28:57 lr 0.000008736	time 0.3514 (0.3570)	dist_loss 0.0422 (0.0396)	image_text_loss 3.8749 (3.8722)	pose_text_loss 2.7743 (3.0527)	tot_loss 7.0708 (7.3210)	mem 29650MB
[2024-12-24 22:18:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6500/11317]	eta 0:28:39 lr 0.000008589	time 0.3672 (0.3570)	dist_loss 0.0356 (0.0397)	image_text_loss 3.8563 (3.8722)	pose_text_loss 3.6212 (3.0507)	tot_loss 7.8337 (7.3198)	mem 29650MB
[2024-12-24 22:19:12 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6550/11317]	eta 0:28:21 lr 0.000008442	time 0.3542 (0.3569)	dist_loss 0.0272 (0.0397)	image_text_loss 3.8714 (3.8721)	pose_text_loss 3.5927 (3.0493)	tot_loss 7.7358 (7.3188)	mem 29650MB
[2024-12-24 22:19:30 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6600/11317]	eta 0:28:03 lr 0.000008296	time 0.3550 (0.3570)	dist_loss 0.0757 (0.0398)	image_text_loss 3.8739 (3.8722)	pose_text_loss 2.3113 (3.0483)	tot_loss 6.9420 (7.3181)	mem 29650MB
[2024-12-24 22:19:48 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6650/11317]	eta 0:27:46 lr 0.000008150	time 0.3514 (0.3570)	dist_loss 0.0451 (0.0399)	image_text_loss 3.8693 (3.8721)	pose_text_loss 2.8170 (3.0460)	tot_loss 7.1375 (7.3167)	mem 29650MB
[2024-12-24 22:20:06 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6700/11317]	eta 0:27:27 lr 0.000008005	time 0.3419 (0.3569)	dist_loss 0.0510 (0.0399)	image_text_loss 3.8475 (3.8721)	pose_text_loss 2.5723 (3.0444)	tot_loss 6.9297 (7.3156)	mem 29650MB
[2024-12-24 22:20:23 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6750/11317]	eta 0:27:10 lr 0.000007861	time 0.3412 (0.3569)	dist_loss 0.0482 (0.0399)	image_text_loss 3.8745 (3.8721)	pose_text_loss 2.7342 (3.0434)	tot_loss 7.0912 (7.3149)	mem 29650MB
[2024-12-24 22:20:41 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6800/11317]	eta 0:26:52 lr 0.000007717	time 0.3516 (0.3569)	dist_loss 0.0170 (0.0400)	image_text_loss 3.8691 (3.8721)	pose_text_loss 3.8292 (3.0424)	tot_loss 7.8683 (7.3141)	mem 29650MB
[2024-12-24 22:20:59 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6850/11317]	eta 0:26:34 lr 0.000007573	time 0.3609 (0.3569)	dist_loss 0.0484 (0.0400)	image_text_loss 3.8939 (3.8721)	pose_text_loss 2.6384 (3.0407)	tot_loss 7.0166 (7.3129)	mem 29650MB
[2024-12-24 22:21:17 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6900/11317]	eta 0:26:16 lr 0.000007431	time 0.3475 (0.3569)	dist_loss 0.0591 (0.0401)	image_text_loss 3.8801 (3.8721)	pose_text_loss 2.5183 (3.0386)	tot_loss 6.9895 (7.3116)	mem 29650MB
[2024-12-24 22:21:34 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][6950/11317]	eta 0:25:58 lr 0.000007289	time 0.3463 (0.3569)	dist_loss 0.0727 (0.0402)	image_text_loss 3.8774 (3.8721)	pose_text_loss 2.3359 (3.0365)	tot_loss 6.9402 (7.3102)	mem 29650MB
[2024-12-24 22:21:52 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7000/11317]	eta 0:25:40 lr 0.000007148	time 0.3595 (0.3569)	dist_loss 0.0447 (0.0402)	image_text_loss 3.8557 (3.8721)	pose_text_loss 2.6274 (3.0349)	tot_loss 6.9305 (7.3091)	mem 29650MB
[2024-12-24 22:22:10 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7050/11317]	eta 0:25:22 lr 0.000007007	time 0.3472 (0.3568)	dist_loss 0.0395 (0.0403)	image_text_loss 3.8778 (3.8721)	pose_text_loss 3.0694 (3.0332)	tot_loss 7.3423 (7.3079)	mem 29650MB
[2024-12-24 22:22:27 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7100/11317]	eta 0:25:04 lr 0.000006868	time 0.3457 (0.3568)	dist_loss 0.0737 (0.0403)	image_text_loss 3.8655 (3.8721)	pose_text_loss 2.3070 (3.0316)	tot_loss 6.9094 (7.3068)	mem 29650MB
[2024-12-24 22:22:45 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7150/11317]	eta 0:24:46 lr 0.000006729	time 0.3518 (0.3568)	dist_loss 0.0366 (0.0404)	image_text_loss 3.8746 (3.8721)	pose_text_loss 2.9254 (3.0302)	tot_loss 7.1662 (7.3058)	mem 29650MB
[2024-12-24 22:23:03 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7200/11317]	eta 0:24:28 lr 0.000006591	time 0.3484 (0.3568)	dist_loss 0.0141 (0.0404)	image_text_loss 3.8787 (3.8720)	pose_text_loss 3.4535 (3.0284)	tot_loss 7.4736 (7.3046)	mem 29650MB
[2024-12-24 22:23:21 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7250/11317]	eta 0:24:10 lr 0.000006454	time 0.3491 (0.3568)	dist_loss 0.0796 (0.0404)	image_text_loss 3.8850 (3.8720)	pose_text_loss 2.2582 (3.0278)	tot_loss 6.9395 (7.3042)	mem 29650MB
[2024-12-24 22:23:38 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7300/11317]	eta 0:23:53 lr 0.000006318	time 0.3664 (0.3568)	dist_loss 0.0245 (0.0405)	image_text_loss 3.8736 (3.8720)	pose_text_loss 3.3901 (3.0267)	tot_loss 7.5088 (7.3034)	mem 29650MB
[2024-12-24 22:23:57 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7350/11317]	eta 0:23:35 lr 0.000006182	time 0.3517 (0.3568)	dist_loss 0.0361 (0.0405)	image_text_loss 3.8756 (3.8720)	pose_text_loss 2.9592 (3.0251)	tot_loss 7.1953 (7.3024)	mem 29650MB
[2024-12-24 22:24:14 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7400/11317]	eta 0:23:17 lr 0.000006048	time 0.3526 (0.3568)	dist_loss 0.0496 (0.0406)	image_text_loss 3.8726 (3.8720)	pose_text_loss 2.6340 (3.0236)	tot_loss 7.0029 (7.3015)	mem 29650MB
[2024-12-24 22:24:32 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7450/11317]	eta 0:22:59 lr 0.000005915	time 0.3635 (0.3568)	dist_loss 0.0222 (0.0406)	image_text_loss 3.8697 (3.8720)	pose_text_loss 3.3431 (3.0232)	tot_loss 7.4353 (7.3010)	mem 29650MB
[2024-12-24 22:24:51 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7500/11317]	eta 0:22:42 lr 0.000005782	time 0.3605 (0.3569)	dist_loss 0.0706 (0.0406)	image_text_loss 3.8877 (3.8720)	pose_text_loss 2.3650 (3.0221)	tot_loss 6.9591 (7.3002)	mem 29650MB
[2024-12-24 22:25:08 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7550/11317]	eta 0:22:24 lr 0.000005651	time 0.3581 (0.3569)	dist_loss 0.0573 (0.0407)	image_text_loss 3.8695 (3.8719)	pose_text_loss 2.5407 (3.0202)	tot_loss 6.9832 (7.2990)	mem 29650MB
[2024-12-24 22:25:26 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7600/11317]	eta 0:22:06 lr 0.000005521	time 0.3568 (0.3568)	dist_loss 0.0429 (0.0407)	image_text_loss 3.8601 (3.8719)	pose_text_loss 2.8319 (3.0188)	tot_loss 7.1207 (7.2979)	mem 29650MB
[2024-12-24 22:25:44 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7650/11317]	eta 0:21:48 lr 0.000005392	time 0.3554 (0.3569)	dist_loss 0.0601 (0.0408)	image_text_loss 3.8625 (3.8719)	pose_text_loss 2.4820 (3.0173)	tot_loss 6.9459 (7.2969)	mem 29650MB
[2024-12-24 22:26:02 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7700/11317]	eta 0:21:30 lr 0.000005264	time 0.3588 (0.3569)	dist_loss 0.0440 (0.0408)	image_text_loss 3.8681 (3.8719)	pose_text_loss 2.8688 (3.0165)	tot_loss 7.1771 (7.2964)	mem 29650MB
[2024-12-24 22:26:20 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7750/11317]	eta 0:21:12 lr 0.000005137	time 0.3465 (0.3568)	dist_loss 0.0560 (0.0408)	image_text_loss 3.8774 (3.8719)	pose_text_loss 2.5247 (3.0153)	tot_loss 6.9616 (7.2955)	mem 29650MB
[2024-12-24 22:26:38 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7800/11317]	eta 0:20:55 lr 0.000005011	time 0.3602 (0.3569)	dist_loss 0.0451 (0.0409)	image_text_loss 3.8711 (3.8719)	pose_text_loss 2.7701 (3.0136)	tot_loss 7.0924 (7.2944)	mem 29650MB
[2024-12-24 22:26:55 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7850/11317]	eta 0:20:37 lr 0.000004886	time 0.3483 (0.3569)	dist_loss 0.0652 (0.0410)	image_text_loss 3.8647 (3.8719)	pose_text_loss 2.4137 (3.0118)	tot_loss 6.9305 (7.2932)	mem 29650MB
[2024-12-24 22:27:13 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7900/11317]	eta 0:20:19 lr 0.000004763	time 0.3460 (0.3568)	dist_loss 0.0484 (0.0410)	image_text_loss 3.8781 (3.8719)	pose_text_loss 2.6339 (3.0105)	tot_loss 6.9960 (7.2922)	mem 29650MB
[2024-12-24 22:27:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][7950/11317]	eta 0:20:01 lr 0.000004640	time 0.3599 (0.3569)	dist_loss 0.0868 (0.0410)	image_text_loss 3.8742 (3.8719)	pose_text_loss 2.1864 (3.0099)	tot_loss 6.9285 (7.2918)	mem 29650MB
[2024-12-24 22:27:49 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8000/11317]	eta 0:19:43 lr 0.000004520	time 0.3693 (0.3569)	dist_loss 0.0568 (0.0411)	image_text_loss 3.8935 (3.8719)	pose_text_loss 2.5362 (3.0085)	tot_loss 6.9980 (7.2910)	mem 29650MB
[2024-12-24 22:28:07 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8050/11317]	eta 0:19:25 lr 0.000004400	time 0.3672 (0.3569)	dist_loss 0.0328 (0.0411)	image_text_loss 3.8753 (3.8718)	pose_text_loss 3.4619 (3.0079)	tot_loss 7.6650 (7.2906)	mem 29650MB
[2024-12-24 22:28:25 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8100/11317]	eta 0:19:08 lr 0.000004281	time 0.3423 (0.3569)	dist_loss 0.0546 (0.0411)	image_text_loss 3.8810 (3.8718)	pose_text_loss 2.6326 (3.0064)	tot_loss 7.0594 (7.2896)	mem 29650MB
[2024-12-24 22:28:43 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8150/11317]	eta 0:18:50 lr 0.000004164	time 0.3758 (0.3569)	dist_loss 0.0186 (0.0412)	image_text_loss 3.8687 (3.8718)	pose_text_loss 3.4580 (3.0053)	tot_loss 7.5129 (7.2888)	mem 29650MB
[2024-12-24 22:29:01 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8200/11317]	eta 0:18:32 lr 0.000004049	time 0.3448 (0.3569)	dist_loss 0.0479 (0.0412)	image_text_loss 3.8710 (3.8718)	pose_text_loss 2.7840 (3.0047)	tot_loss 7.1342 (7.2883)	mem 29650MB
[2024-12-24 22:29:19 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8250/11317]	eta 0:18:14 lr 0.000003934	time 0.3586 (0.3569)	dist_loss 0.0243 (0.0412)	image_text_loss 3.8719 (3.8718)	pose_text_loss 3.1200 (3.0031)	tot_loss 7.2353 (7.2872)	mem 29650MB
[2024-12-24 22:29:36 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8300/11317]	eta 0:17:56 lr 0.000003821	time 0.3448 (0.3569)	dist_loss 0.0218 (0.0412)	image_text_loss 3.8766 (3.8718)	pose_text_loss 3.3393 (3.0022)	tot_loss 7.4344 (7.2865)	mem 29650MB
[2024-12-24 22:29:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8350/11317]	eta 0:17:38 lr 0.000003710	time 0.3518 (0.3569)	dist_loss 0.0455 (0.0413)	image_text_loss 3.8408 (3.8718)	pose_text_loss 2.6368 (3.0017)	tot_loss 6.9325 (7.2861)	mem 29650MB
[2024-12-24 22:30:12 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8400/11317]	eta 0:17:21 lr 0.000003599	time 0.3574 (0.3569)	dist_loss 0.0347 (0.0413)	image_text_loss 3.8942 (3.8718)	pose_text_loss 3.0754 (3.0012)	tot_loss 7.3164 (7.2857)	mem 29650MB
[2024-12-24 22:30:30 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8450/11317]	eta 0:17:03 lr 0.000003491	time 0.3538 (0.3569)	dist_loss 0.0559 (0.0413)	image_text_loss 3.8531 (3.8718)	pose_text_loss 2.5007 (3.0004)	tot_loss 6.9130 (7.2851)	mem 29650MB
[2024-12-24 22:30:48 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8500/11317]	eta 0:16:45 lr 0.000003383	time 0.3572 (0.3569)	dist_loss 0.0561 (0.0413)	image_text_loss 3.9163 (3.8718)	pose_text_loss 2.5883 (2.9988)	tot_loss 7.0655 (7.2840)	mem 29650MB
[2024-12-24 22:31:06 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8550/11317]	eta 0:16:27 lr 0.000003278	time 0.3512 (0.3570)	dist_loss 0.0433 (0.0414)	image_text_loss 3.8611 (3.8718)	pose_text_loss 3.8967 (2.9980)	tot_loss 8.1909 (7.2835)	mem 29650MB
[2024-12-24 22:31:24 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8600/11317]	eta 0:16:09 lr 0.000003173	time 0.3575 (0.3569)	dist_loss 0.0319 (0.0414)	image_text_loss 3.8731 (3.8718)	pose_text_loss 3.1211 (2.9970)	tot_loss 7.3127 (7.2828)	mem 29650MB
[2024-12-24 22:31:42 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8650/11317]	eta 0:15:51 lr 0.000003071	time 0.3529 (0.3569)	dist_loss 0.0427 (0.0414)	image_text_loss 3.8943 (3.8718)	pose_text_loss 2.8976 (2.9959)	tot_loss 7.2187 (7.2820)	mem 29650MB
[2024-12-24 22:32:00 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8700/11317]	eta 0:15:34 lr 0.000002970	time 0.3687 (0.3570)	dist_loss 0.0407 (0.0415)	image_text_loss 3.8711 (3.8718)	pose_text_loss 2.8697 (2.9953)	tot_loss 7.1475 (7.2816)	mem 29650MB
[2024-12-24 22:32:17 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8750/11317]	eta 0:15:16 lr 0.000002870	time 0.3680 (0.3569)	dist_loss 0.0245 (0.0415)	image_text_loss 3.8769 (3.8718)	pose_text_loss 3.1854 (2.9948)	tot_loss 7.3073 (7.2812)	mem 29650MB
[2024-12-24 22:32:35 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8800/11317]	eta 0:14:58 lr 0.000002772	time 0.3455 (0.3569)	dist_loss 0.0534 (0.0415)	image_text_loss 3.8657 (3.8718)	pose_text_loss 2.5519 (2.9938)	tot_loss 6.9518 (7.2805)	mem 29650MB
[2024-12-24 22:32:53 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8850/11317]	eta 0:14:40 lr 0.000002675	time 0.3518 (0.3569)	dist_loss 0.0513 (0.0415)	image_text_loss 3.8669 (3.8717)	pose_text_loss 2.7089 (2.9929)	tot_loss 7.0887 (7.2799)	mem 29650MB
[2024-12-24 22:33:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8900/11317]	eta 0:14:22 lr 0.000002581	time 0.3507 (0.3569)	dist_loss 0.0535 (0.0415)	image_text_loss 3.8705 (3.8717)	pose_text_loss 2.5620 (2.9921)	tot_loss 6.9677 (7.2792)	mem 29650MB
[2024-12-24 22:33:28 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][8950/11317]	eta 0:14:04 lr 0.000002488	time 0.3447 (0.3569)	dist_loss 0.0388 (0.0415)	image_text_loss 3.8699 (3.8717)	pose_text_loss 3.2854 (2.9917)	tot_loss 7.5431 (7.2788)	mem 29650MB
[2024-12-24 22:33:47 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9000/11317]	eta 0:13:47 lr 0.000002396	time 0.3536 (0.3569)	dist_loss 0.0722 (0.0416)	image_text_loss 3.8678 (3.8717)	pose_text_loss 2.3459 (2.9904)	tot_loss 6.9360 (7.2780)	mem 29650MB
[2024-12-24 22:34:04 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9050/11317]	eta 0:13:29 lr 0.000002306	time 0.3501 (0.3569)	dist_loss 0.0448 (0.0416)	image_text_loss 3.8709 (3.8717)	pose_text_loss 2.7759 (2.9894)	tot_loss 7.0952 (7.2773)	mem 29650MB
[2024-12-24 22:34:22 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9100/11317]	eta 0:13:11 lr 0.000002218	time 0.3508 (0.3569)	dist_loss 0.0539 (0.0417)	image_text_loss 3.8494 (3.8717)	pose_text_loss 2.5720 (2.9882)	tot_loss 6.9605 (7.2764)	mem 29650MB
[2024-12-24 22:34:40 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9150/11317]	eta 0:12:53 lr 0.000002132	time 0.3455 (0.3569)	dist_loss 0.0623 (0.0417)	image_text_loss 3.8573 (3.8717)	pose_text_loss 2.4583 (2.9872)	tot_loss 6.9390 (7.2757)	mem 29650MB
[2024-12-24 22:34:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9200/11317]	eta 0:12:35 lr 0.000002047	time 0.3599 (0.3569)	dist_loss 0.0380 (0.0417)	image_text_loss 3.8662 (3.8717)	pose_text_loss 2.9492 (2.9857)	tot_loss 7.1956 (7.2747)	mem 29650MB
[2024-12-24 22:35:16 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9250/11317]	eta 0:12:17 lr 0.000001964	time 0.3580 (0.3569)	dist_loss 0.0274 (0.0418)	image_text_loss 3.8770 (3.8717)	pose_text_loss 3.2521 (2.9849)	tot_loss 7.4037 (7.2741)	mem 29650MB
[2024-12-24 22:35:33 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9300/11317]	eta 0:11:59 lr 0.000001883	time 0.3347 (0.3569)	dist_loss 0.0492 (0.0418)	image_text_loss 3.9237 (3.8717)	pose_text_loss 2.7683 (2.9839)	tot_loss 7.1844 (7.2735)	mem 29650MB
[2024-12-24 22:35:51 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9350/11317]	eta 0:11:41 lr 0.000001804	time 0.3529 (0.3569)	dist_loss 0.0749 (0.0418)	image_text_loss 3.8706 (3.8717)	pose_text_loss 2.2664 (2.9831)	tot_loss 6.8856 (7.2729)	mem 29650MB
[2024-12-24 22:36:09 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9400/11317]	eta 0:11:24 lr 0.000001726	time 0.3525 (0.3569)	dist_loss 0.0655 (0.0418)	image_text_loss 3.8947 (3.8717)	pose_text_loss 2.4554 (2.9823)	tot_loss 7.0049 (7.2725)	mem 29650MB
[2024-12-24 22:36:27 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9450/11317]	eta 0:11:06 lr 0.000001650	time 0.3531 (0.3569)	dist_loss 0.0455 (0.0419)	image_text_loss 3.8687 (3.8717)	pose_text_loss 2.7825 (2.9814)	tot_loss 7.1062 (7.2719)	mem 29650MB
[2024-12-24 22:36:44 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9500/11317]	eta 0:10:48 lr 0.000001576	time 0.3617 (0.3569)	dist_loss 0.0781 (0.0419)	image_text_loss 3.8720 (3.8717)	pose_text_loss 2.2857 (2.9802)	tot_loss 6.9386 (7.2713)	mem 29650MB
[2024-12-24 22:37:02 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9550/11317]	eta 0:10:30 lr 0.000001504	time 0.3557 (0.3569)	dist_loss 0.0543 (0.0420)	image_text_loss 3.8743 (3.8717)	pose_text_loss 2.5426 (2.9793)	tot_loss 6.9595 (7.2707)	mem 29650MB
[2024-12-24 22:37:20 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9600/11317]	eta 0:10:12 lr 0.000001434	time 0.3814 (0.3569)	dist_loss 0.0644 (0.0420)	image_text_loss 3.8696 (3.8716)	pose_text_loss 2.4217 (2.9778)	tot_loss 6.9349 (7.2696)	mem 29650MB
[2024-12-24 22:37:38 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9650/11317]	eta 0:09:54 lr 0.000001365	time 0.3561 (0.3569)	dist_loss 0.0776 (0.0421)	image_text_loss 3.8631 (3.8716)	pose_text_loss 2.2492 (2.9765)	tot_loss 6.8887 (7.2688)	mem 29650MB
[2024-12-24 22:37:56 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9700/11317]	eta 0:09:37 lr 0.000001299	time 0.3444 (0.3568)	dist_loss 0.0690 (0.0421)	image_text_loss 3.8735 (3.8716)	pose_text_loss 2.3584 (2.9754)	tot_loss 6.9216 (7.2681)	mem 29650MB
[2024-12-24 22:38:13 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9750/11317]	eta 0:09:19 lr 0.000001234	time 0.3478 (0.3569)	dist_loss 0.0279 (0.0421)	image_text_loss 3.8617 (3.8716)	pose_text_loss 2.9803 (2.9747)	tot_loss 7.1211 (7.2676)	mem 29650MB
[2024-12-24 22:38:31 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9800/11317]	eta 0:09:01 lr 0.000001171	time 0.3494 (0.3568)	dist_loss 0.0654 (0.0422)	image_text_loss 3.8577 (3.8716)	pose_text_loss 2.3890 (2.9737)	tot_loss 6.9004 (7.2668)	mem 29650MB
[2024-12-24 22:38:49 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9850/11317]	eta 0:08:43 lr 0.000001111	time 0.3562 (0.3568)	dist_loss 0.0246 (0.0422)	image_text_loss 3.8865 (3.8716)	pose_text_loss 3.4990 (2.9730)	tot_loss 7.6317 (7.2662)	mem 29650MB
[2024-12-24 22:39:07 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9900/11317]	eta 0:08:25 lr 0.000001052	time 0.3625 (0.3569)	dist_loss 0.0359 (0.0422)	image_text_loss 3.8670 (3.8716)	pose_text_loss 2.8331 (2.9722)	tot_loss 7.0586 (7.2656)	mem 29650MB
[2024-12-24 22:39:25 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][9950/11317]	eta 0:08:07 lr 0.000000995	time 0.3481 (0.3569)	dist_loss 0.0568 (0.0422)	image_text_loss 3.8671 (3.8716)	pose_text_loss 2.5447 (2.9713)	tot_loss 6.9794 (7.2650)	mem 29650MB
[2024-12-24 22:39:43 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10000/11317]	eta 0:07:49 lr 0.000000940	time 0.3609 (0.3569)	dist_loss 0.0747 (0.0422)	image_text_loss 3.8728 (3.8716)	pose_text_loss 2.2787 (2.9704)	tot_loss 6.8981 (7.2643)	mem 29650MB
[2024-12-24 22:40:01 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10050/11317]	eta 0:07:32 lr 0.000000887	time 0.3606 (0.3569)	dist_loss 0.0216 (0.0423)	image_text_loss 3.8700 (3.8716)	pose_text_loss 3.3262 (2.9698)	tot_loss 7.4125 (7.2639)	mem 29650MB
[2024-12-24 22:40:19 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10100/11317]	eta 0:07:14 lr 0.000000836	time 0.3520 (0.3569)	dist_loss 0.0541 (0.0422)	image_text_loss 3.8700 (3.8715)	pose_text_loss 2.5598 (2.9696)	tot_loss 6.9708 (7.2636)	mem 29650MB
[2024-12-24 22:40:36 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10150/11317]	eta 0:06:56 lr 0.000000786	time 0.3529 (0.3569)	dist_loss 0.0180 (0.0423)	image_text_loss 3.8783 (3.8715)	pose_text_loss 3.2329 (2.9686)	tot_loss 7.2914 (7.2628)	mem 29650MB
[2024-12-24 22:40:54 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10200/11317]	eta 0:06:38 lr 0.000000739	time 0.3736 (0.3569)	dist_loss 0.0343 (0.0423)	image_text_loss 3.8764 (3.8715)	pose_text_loss 3.0352 (2.9677)	tot_loss 7.2543 (7.2623)	mem 29650MB
[2024-12-24 22:41:12 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10250/11317]	eta 0:06:20 lr 0.000000694	time 0.3705 (0.3569)	dist_loss 0.0732 (0.0423)	image_text_loss 3.8880 (3.8715)	pose_text_loss 2.3045 (2.9671)	tot_loss 6.9248 (7.2619)	mem 29650MB
[2024-12-24 22:41:30 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10300/11317]	eta 0:06:02 lr 0.000000651	time 0.3642 (0.3569)	dist_loss 0.0478 (0.0424)	image_text_loss 3.8683 (3.8715)	pose_text_loss 2.6654 (2.9660)	tot_loss 7.0119 (7.2612)	mem 29650MB
[2024-12-24 22:41:48 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10350/11317]	eta 0:05:45 lr 0.000000610	time 0.3383 (0.3569)	dist_loss 0.0512 (0.0424)	image_text_loss 3.8816 (3.8715)	pose_text_loss 2.5965 (2.9651)	tot_loss 6.9904 (7.2605)	mem 29650MB
[2024-12-24 22:42:05 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10400/11317]	eta 0:05:27 lr 0.000000571	time 0.3677 (0.3569)	dist_loss 0.0290 (0.0424)	image_text_loss 3.8720 (3.8715)	pose_text_loss 3.1946 (2.9645)	tot_loss 7.3571 (7.2600)	mem 29650MB
[2024-12-24 22:42:23 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10450/11317]	eta 0:05:09 lr 0.000000534	time 0.3592 (0.3569)	dist_loss 0.0497 (0.0424)	image_text_loss 3.8574 (3.8715)	pose_text_loss 2.6472 (2.9635)	tot_loss 7.0019 (7.2594)	mem 29650MB
[2024-12-24 22:42:41 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10500/11317]	eta 0:04:51 lr 0.000000499	time 0.3492 (0.3569)	dist_loss 0.0335 (0.0425)	image_text_loss 3.8714 (3.8715)	pose_text_loss 3.3687 (2.9624)	tot_loss 7.5750 (7.2588)	mem 29650MB
[2024-12-24 22:42:59 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10550/11317]	eta 0:04:33 lr 0.000000466	time 0.3625 (0.3569)	dist_loss 0.0305 (0.0425)	image_text_loss 3.8716 (3.8715)	pose_text_loss 3.4193 (2.9618)	tot_loss 7.5956 (7.2583)	mem 29650MB
[2024-12-24 22:43:17 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10600/11317]	eta 0:04:15 lr 0.000000435	time 0.3539 (0.3569)	dist_loss 0.0762 (0.0425)	image_text_loss 3.8643 (3.8715)	pose_text_loss 2.2781 (2.9610)	tot_loss 6.9046 (7.2578)	mem 29650MB
[2024-12-24 22:43:35 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10650/11317]	eta 0:03:58 lr 0.000000406	time 0.3547 (0.3569)	dist_loss 0.0856 (0.0426)	image_text_loss 3.8878 (3.8715)	pose_text_loss 2.2050 (2.9603)	tot_loss 6.9483 (7.2573)	mem 29650MB
[2024-12-24 22:43:53 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10700/11317]	eta 0:03:40 lr 0.000000379	time 0.3552 (0.3569)	dist_loss 0.0495 (0.0426)	image_text_loss 3.8597 (3.8714)	pose_text_loss 2.6752 (2.9596)	tot_loss 7.0302 (7.2568)	mem 29650MB
[2024-12-24 22:44:11 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10750/11317]	eta 0:03:22 lr 0.000000355	time 0.3491 (0.3569)	dist_loss 0.0486 (0.0426)	image_text_loss 3.9337 (3.8714)	pose_text_loss 2.7428 (2.9585)	tot_loss 7.1621 (7.2561)	mem 29650MB
[2024-12-24 22:44:29 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10800/11317]	eta 0:03:04 lr 0.000000332	time 0.3550 (0.3569)	dist_loss 0.0790 (0.0426)	image_text_loss 3.8610 (3.8714)	pose_text_loss 2.2304 (2.9579)	tot_loss 6.8815 (7.2556)	mem 29650MB
[2024-12-24 22:44:47 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10850/11317]	eta 0:02:46 lr 0.000000311	time 0.3608 (0.3569)	dist_loss 0.0437 (0.0427)	image_text_loss 3.8662 (3.8714)	pose_text_loss 2.8240 (2.9570)	tot_loss 7.1270 (7.2551)	mem 29650MB
[2024-12-24 22:45:04 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10900/11317]	eta 0:02:28 lr 0.000000293	time 0.3523 (0.3569)	dist_loss 0.0509 (0.0427)	image_text_loss 3.8787 (3.8714)	pose_text_loss 2.7075 (2.9563)	tot_loss 7.0951 (7.2545)	mem 29650MB
[2024-12-24 22:45:22 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][10950/11317]	eta 0:02:10 lr 0.000000276	time 0.3614 (0.3569)	dist_loss 0.0407 (0.0427)	image_text_loss 3.8711 (3.8714)	pose_text_loss 2.8178 (2.9551)	tot_loss 7.0960 (7.2538)	mem 29650MB
[2024-12-24 22:45:40 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11000/11317]	eta 0:01:53 lr 0.000000262	time 0.3620 (0.3569)	dist_loss 0.0291 (0.0427)	image_text_loss 3.8686 (3.8714)	pose_text_loss 3.1273 (2.9551)	tot_loss 7.2868 (7.2536)	mem 29650MB
[2024-12-24 22:45:58 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11050/11317]	eta 0:01:35 lr 0.000000250	time 0.4232 (0.3569)	dist_loss 0.0614 (0.0427)	image_text_loss 3.8718 (3.8714)	pose_text_loss 2.4724 (2.9546)	tot_loss 6.9579 (7.2533)	mem 29650MB
[2024-12-24 22:46:16 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11100/11317]	eta 0:01:17 lr 0.000000240	time 0.3584 (0.3569)	dist_loss 0.0781 (0.0428)	image_text_loss 3.8696 (3.8714)	pose_text_loss 2.2527 (2.9538)	tot_loss 6.9032 (7.2528)	mem 29650MB
[2024-12-24 22:46:34 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11150/11317]	eta 0:00:59 lr 0.000000232	time 0.3520 (0.3569)	dist_loss 0.0540 (0.0428)	image_text_loss 3.8620 (3.8714)	pose_text_loss 2.6015 (2.9531)	tot_loss 7.0038 (7.2522)	mem 29650MB
[2024-12-24 22:46:52 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11200/11317]	eta 0:00:41 lr 0.000000226	time 0.3727 (0.3569)	dist_loss 0.0526 (0.0428)	image_text_loss 3.8660 (3.8714)	pose_text_loss 2.5483 (2.9525)	tot_loss 6.9403 (7.2518)	mem 29650MB
[2024-12-24 22:47:10 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11250/11317]	eta 0:00:23 lr 0.000000222	time 0.3495 (0.3569)	dist_loss 0.0253 (0.0428)	image_text_loss 3.8769 (3.8714)	pose_text_loss 3.3634 (2.9517)	tot_loss 7.4933 (7.2512)	mem 29650MB
[2024-12-24 22:47:28 ViT-B/16] (CL_main_v2.py 273): INFO Train: [0/1][11300/11317]	eta 0:00:06 lr 0.000000220	time 0.3521 (0.3569)	dist_loss 0.0385 (0.0428)	image_text_loss 3.8638 (3.8713)	pose_text_loss 2.8803 (2.9509)	tot_loss 7.1286 (7.2507)	mem 29650MB
[2024-12-24 22:47:33 ViT-B/16] (CL_main_v2.py 290): INFO EPOCH 0 training takes 1:07:19
[2024-12-24 22:47:33 ViT-B/16] (CL_main_v2.py 307): INFO 1 views inference
[2024-12-24 22:47:35 ViT-B/16] (CL_main_v2.py 379): INFO Test: [0/250]	Acc@1 for Image-Text: 50.000	mCA for Image-Text: 2.083	
[2024-12-24 22:47:35 ViT-B/16] (CL_main_v2.py 379): INFO Test: [0/250]	Acc@1 for Text-Pose: 50.000	mCA for Text-Pose: 2.083	
[2024-12-24 22:47:35 ViT-B/16] (CL_main_v2.py 379): INFO Test: [0/250]	Acc@1 for Image-Text-Pose: 50.000	mCA for Image-Text-Pose: 2.083	
[2024-12-24 22:47:42 ViT-B/16] (CL_main_v2.py 379): INFO Test: [50/250]	Acc@1 for Image-Text: 4.902	mCA for Image-Text: 3.299	
[2024-12-24 22:47:42 ViT-B/16] (CL_main_v2.py 379): INFO Test: [50/250]	Acc@1 for Text-Pose: 72.549	mCA for Text-Pose: 70.263	
[2024-12-24 22:47:42 ViT-B/16] (CL_main_v2.py 379): INFO Test: [50/250]	Acc@1 for Image-Text-Pose: 72.549	mCA for Image-Text-Pose: 70.263	
[2024-12-24 22:47:49 ViT-B/16] (CL_main_v2.py 379): INFO Test: [100/250]	Acc@1 for Image-Text: 4.455	mCA for Image-Text: 3.038	
[2024-12-24 22:47:49 ViT-B/16] (CL_main_v2.py 379): INFO Test: [100/250]	Acc@1 for Text-Pose: 70.792	mCA for Text-Pose: 72.374	
[2024-12-24 22:47:49 ViT-B/16] (CL_main_v2.py 379): INFO Test: [100/250]	Acc@1 for Image-Text-Pose: 70.792	mCA for Image-Text-Pose: 72.374	
[2024-12-24 22:47:56 ViT-B/16] (CL_main_v2.py 379): INFO Test: [150/250]	Acc@1 for Image-Text: 3.311	mCA for Image-Text: 2.626	
[2024-12-24 22:47:56 ViT-B/16] (CL_main_v2.py 379): INFO Test: [150/250]	Acc@1 for Text-Pose: 70.199	mCA for Text-Pose: 72.249	
[2024-12-24 22:47:56 ViT-B/16] (CL_main_v2.py 379): INFO Test: [150/250]	Acc@1 for Image-Text-Pose: 70.199	mCA for Image-Text-Pose: 72.249	
[2024-12-24 22:48:02 ViT-B/16] (CL_main_v2.py 379): INFO Test: [200/250]	Acc@1 for Image-Text: 2.985	mCA for Image-Text: 2.280	
[2024-12-24 22:48:02 ViT-B/16] (CL_main_v2.py 379): INFO Test: [200/250]	Acc@1 for Text-Pose: 71.144	mCA for Text-Pose: 72.947	
[2024-12-24 22:48:02 ViT-B/16] (CL_main_v2.py 379): INFO Test: [200/250]	Acc@1 for Image-Text-Pose: 70.896	mCA for Image-Text-Pose: 72.649	
[2024-12-24 22:48:10 ViT-B/16] (CL_main_v2.py 406): INFO  * Acc@1 for Image-Text: 3.150 Acc@5 for Image-Text: 12.600 mCA for Image-Text: 2.885
[2024-12-24 22:48:10 ViT-B/16] (CL_main_v2.py 406): INFO  * Acc@1 for Text-Pose: 70.550 Acc@5 for Text-Pose: 86.350 mCA for Text-Pose: 71.699
[2024-12-24 22:48:10 ViT-B/16] (CL_main_v2.py 406): INFO  * Acc@1 for Image-Text-Pose: 70.550 Acc@5 for Image-Text-Pose: 86.400 mCA for Image-Text-Pose: 71.696
[2024-12-24 22:48:10 ViT-B/16] (CL_main_v2.py 157): INFO Accuracy of the network on the 2004 test videos on Image-Text: 3.1%. mCA: 2.9
[2024-12-24 22:48:10 ViT-B/16] (CL_main_v2.py 165): INFO Max accuracy: 3.15%
[2024-12-24 22:48:10 ViT-B/16] (tools.py 93): INFO ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/ckpt_epoch_0.pth saving......
[2024-12-24 22:48:13 ViT-B/16] (tools.py 95): INFO ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/ckpt_epoch_0.pth saved !!!
[2024-12-24 22:48:16 ViT-B/16] (tools.py 99): INFO ./workdirs/TEST_P2_SKIViFiCLIP_zs_48_25Dec24/best.pth saved !!!
